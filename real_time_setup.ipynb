{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662f1509",
   "metadata": {},
   "source": [
    "## Conversation Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5810455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Conversation Simulator\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SpeakerTurn:\n",
    "    \"\"\"Represents one speaker's turn in conversation\"\"\"\n",
    "\n",
    "    speaker_id: str\n",
    "    audio_path: str\n",
    "    transcript: str\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    overlap_with_previous: float = 0.0\n",
    "    overlap_with_next: float = 0.0\n",
    "\n",
    "\n",
    "class ConversationSimulator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        librispeech_root: str,\n",
    "        output_dir: str,\n",
    "        num_speakers: int = 3,\n",
    "        turns_per_speaker: int = 3,\n",
    "        sr: int = 16000,\n",
    "        min_overlap_sec: float = 0.5,\n",
    "        max_overlap_sec: float = 2.0,\n",
    "        silence_between_speakers: float = 0.2,\n",
    "        clean_output_dir: bool = True,\n",
    "    ):\n",
    "\n",
    "        self.librispeech_root = Path(librispeech_root)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.num_speakers = num_speakers\n",
    "        self.turns_per_speaker = turns_per_speaker\n",
    "        self.sr = sr\n",
    "        self.min_overlap_sec = min_overlap_sec\n",
    "        self.max_overlap_sec = max_overlap_sec\n",
    "        self.silence_between_speakers = silence_between_speakers\n",
    "\n",
    "        # Clean and create output directories\n",
    "        if clean_output_dir and self.output_dir.exists():\n",
    "            print(f\"Cleaning existing output directory: {self.output_dir}\")\n",
    "            shutil.rmtree(self.output_dir)\n",
    "\n",
    "        # Create output directories (with parents)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        (self.output_dir / \"conversations\").mkdir(exist_ok=True)\n",
    "        (self.output_dir / \"reference\").mkdir(exist_ok=True)\n",
    "\n",
    "        # Cache for speaker analysis to speed up multiple conversations\n",
    "        self._speaker_cache = None\n",
    "\n",
    "    def get_transcript(self, flac_path):\n",
    "        \"\"\"Extract transcript from LibriSpeech structure\"\"\"\n",
    "        flac_path = Path(flac_path)\n",
    "        speaker_id = flac_path.parent.parent.name\n",
    "        chapter_id = flac_path.parent.name\n",
    "        txt_path = flac_path.parent / f\"{speaker_id}-{chapter_id}.trans.txt\"\n",
    "\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                search_key = f\"{flac_path.stem} \"\n",
    "                for line in f:\n",
    "                    if line.startswith(search_key):\n",
    "                        return line[len(search_key) :].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Transcript error for {flac_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "    def _analyze_speakers_once(self) -> List[Tuple[str, List[Path]]]:\n",
    "        \"\"\"Analyze all speakers once and cache results\"\"\"\n",
    "        if self._speaker_cache is not None:\n",
    "            return self._speaker_cache\n",
    "\n",
    "        print(\"Analyzing LibriSpeech speakers (one-time setup)...\")\n",
    "        speaker_dirs = []\n",
    "\n",
    "        # Check if we have LibriSpeech subdirectories (test-clean, train-clean-100, etc.)\n",
    "        potential_roots = [self.librispeech_root]\n",
    "\n",
    "        # Check for common LibriSpeech subdirectories\n",
    "        for subdir in [\n",
    "            \"test-clean\",\n",
    "            \"dev-clean\",\n",
    "            \"train-clean-100\",\n",
    "            \"train-clean-360\",\n",
    "            \"train-other-500\",\n",
    "        ]:\n",
    "            subdir_path = self.librispeech_root / subdir\n",
    "            if subdir_path.exists() and subdir_path.is_dir():\n",
    "                potential_roots.append(subdir_path)\n",
    "                print(f\"Found LibriSpeech subset: {subdir}\")\n",
    "\n",
    "        # If we found subdirectories, use them instead of root\n",
    "        if len(potential_roots) > 1:\n",
    "            search_roots = potential_roots[1:]  # Skip the original root\n",
    "        else:\n",
    "            search_roots = [self.librispeech_root]\n",
    "\n",
    "        print(f\"Searching in: {[str(root) for root in search_roots]}\")\n",
    "\n",
    "        # Find all speaker directories\n",
    "        for search_root in search_roots:\n",
    "            print(f\"Scanning directory: {search_root}\")\n",
    "\n",
    "            try:\n",
    "                speaker_list = list(os.listdir(search_root))\n",
    "                print(f\"Found {len(speaker_list)} entries in {search_root}\")\n",
    "            except:\n",
    "                print(f\"Cannot read directory: {search_root}\")\n",
    "                continue\n",
    "\n",
    "            for speaker in speaker_list:\n",
    "                speaker_path = search_root / speaker\n",
    "                if not speaker_path.is_dir():\n",
    "                    continue\n",
    "\n",
    "                # Get all flac files for this speaker\n",
    "                flac_files = list(speaker_path.rglob(\"*.flac\"))\n",
    "\n",
    "                # Sample a few files to check durations (faster than checking all)\n",
    "                sample_size = min(5, len(flac_files))\n",
    "                sample_files = random.sample(flac_files, sample_size)\n",
    "                valid_count = 0\n",
    "\n",
    "                for flac_path in sample_files:\n",
    "                    try:\n",
    "                        # Fixed: use 'path' instead of 'filename'\n",
    "                        duration = librosa.get_duration(path=flac_path)\n",
    "                        if duration >= 1.0:  # At least 1 second\n",
    "                            valid_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error checking {flac_path}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # If at least half the sample is valid, include this speaker\n",
    "                if valid_count >= sample_size // 2:\n",
    "                    speaker_dirs.append((speaker, flac_files))\n",
    "\n",
    "        self._speaker_cache = speaker_dirs\n",
    "        return speaker_dirs\n",
    "\n",
    "    def select_speakers_and_files(self) -> Dict[str, List[Path]]:\n",
    "        \"\"\"Select speakers and their audio files (optimized)\"\"\"\n",
    "        all_speakers = self._analyze_speakers_once()\n",
    "\n",
    "        if len(all_speakers) < self.num_speakers:\n",
    "            raise ValueError(\n",
    "                f\"Not enough speakers found. Need {self.num_speakers}, found {len(all_speakers)}\"\n",
    "            )\n",
    "\n",
    "        # Select required number of speakers\n",
    "        selected_speakers = random.sample(all_speakers, self.num_speakers)\n",
    "\n",
    "        # Select and validate audio files for each speaker\n",
    "        audio_pool = {}\n",
    "        for speaker_id, all_files in selected_speakers:\n",
    "            # Shuffle and check files until we have enough valid ones\n",
    "            shuffled_files = random.sample(\n",
    "                all_files, min(len(all_files), self.turns_per_speaker * 3)\n",
    "            )\n",
    "            valid_files = []\n",
    "\n",
    "            for flac_path in shuffled_files:\n",
    "                if len(valid_files) >= self.turns_per_speaker:\n",
    "                    break\n",
    "                try:\n",
    "                    # Only check duration for files we actually plan to use\n",
    "                    if librosa.get_duration(path=flac_path) >= 1.0:  # Lowered threshold\n",
    "                        valid_files.append(flac_path)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if len(valid_files) < self.turns_per_speaker:\n",
    "                # Fallback: use what we have\n",
    "                print(\n",
    "                    f\"Warning: Speaker {speaker_id} only has {len(valid_files)} valid files\"\n",
    "                )\n",
    "\n",
    "            audio_pool[speaker_id] = valid_files[: self.turns_per_speaker]\n",
    "\n",
    "        return audio_pool\n",
    "\n",
    "    def create_reference_audios(\n",
    "        self, audio_pool: Dict[str, List[Path]]\n",
    "    ) -> Dict[str, Path]:\n",
    "        \"\"\"Create reference audio files for each speaker\"\"\"\n",
    "        ref_audios = {}\n",
    "\n",
    "        for speaker_id, files in audio_pool.items():\n",
    "            # Choose the longest file as reference\n",
    "            ref_file = max(files, key=lambda x: librosa.get_duration(filename=x))\n",
    "            ref_path = self.output_dir / \"reference\" / f\"{speaker_id}.wav\"\n",
    "\n",
    "            # Load and save as reference\n",
    "            y, _ = librosa.load(ref_file, sr=self.sr)\n",
    "            sf.write(ref_path, y, self.sr)\n",
    "            ref_audios[speaker_id] = ref_path\n",
    "\n",
    "        return ref_audios\n",
    "\n",
    "    def generate_conversation_sequence(\n",
    "        self, audio_pool: Dict[str, List[Path]]\n",
    "    ) -> List[SpeakerTurn]:\n",
    "        \"\"\"Generate conversation sequence with speaker turns\"\"\"\n",
    "        turns = []\n",
    "        speakers = list(audio_pool.keys())\n",
    "\n",
    "        # Create round-robin conversation pattern\n",
    "        # Each speaker gets their turns distributed throughout the conversation\n",
    "        all_turns = []\n",
    "        for speaker_id in speakers:\n",
    "            for i, audio_path in enumerate(audio_pool[speaker_id]):\n",
    "                transcript = self.get_transcript(audio_path)\n",
    "                all_turns.append(\n",
    "                    {\n",
    "                        \"speaker_id\": speaker_id,\n",
    "                        \"audio_path\": audio_path,\n",
    "                        \"transcript\": transcript,\n",
    "                        \"speaker_turn_index\": i,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Shuffle to create natural conversation flow\n",
    "        random.shuffle(all_turns)\n",
    "\n",
    "        # Convert to SpeakerTurn objects with timing\n",
    "        current_time = 0.0\n",
    "\n",
    "        for i, turn_data in enumerate(all_turns):\n",
    "            # Load audio to get duration\n",
    "            audio_path = turn_data[\"audio_path\"]\n",
    "            y, _ = librosa.load(audio_path, sr=self.sr)\n",
    "            duration = len(y) / self.sr\n",
    "\n",
    "            # Calculate overlap with previous speaker (only if different speaker)\n",
    "            overlap_with_prev = 0.0\n",
    "            if i > 0 and turns[i - 1].speaker_id != turn_data[\"speaker_id\"]:\n",
    "                overlap_with_prev = random.uniform(\n",
    "                    self.min_overlap_sec,\n",
    "                    min(\n",
    "                        self.max_overlap_sec,\n",
    "                        duration * 0.3,  # Max 30% of current audio\n",
    "                        turns[i - 1].end_time - turns[i - 1].start_time,\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            # Adjust start time based on overlap\n",
    "            if overlap_with_prev > 0:\n",
    "                start_time = current_time - overlap_with_prev\n",
    "            else:\n",
    "                start_time = current_time + self.silence_between_speakers\n",
    "\n",
    "            end_time = start_time + duration\n",
    "\n",
    "            turn = SpeakerTurn(\n",
    "                speaker_id=turn_data[\"speaker_id\"],\n",
    "                audio_path=audio_path,\n",
    "                transcript=turn_data[\"transcript\"],\n",
    "                start_time=start_time,\n",
    "                end_time=end_time,\n",
    "                overlap_with_previous=overlap_with_prev,\n",
    "            )\n",
    "\n",
    "            turns.append(turn)\n",
    "            current_time = end_time\n",
    "\n",
    "        return turns\n",
    "\n",
    "    def create_conversation_audio(\n",
    "        self, turns: List[SpeakerTurn], output_path: Path\n",
    "    ) -> float:\n",
    "        \"\"\"Create the final conversation audio file\"\"\"\n",
    "        # Calculate total duration\n",
    "        total_duration = max(turn.end_time for turn in turns)\n",
    "        total_samples = int(total_duration * self.sr)\n",
    "\n",
    "        # Create empty audio array\n",
    "        conversation_audio = np.zeros(total_samples)\n",
    "\n",
    "        # Add each speaker's audio\n",
    "        for turn in turns:\n",
    "            # Load audio\n",
    "            y, _ = librosa.load(turn.audio_path, sr=self.sr)\n",
    "\n",
    "            # Calculate sample positions\n",
    "            start_sample = int(turn.start_time * self.sr)\n",
    "            end_sample = min(start_sample + len(y), total_samples)\n",
    "            audio_length = end_sample - start_sample\n",
    "\n",
    "            # Add to conversation (with overlap handling)\n",
    "            if audio_length > 0:\n",
    "                conversation_audio[start_sample:end_sample] += y[:audio_length]\n",
    "\n",
    "        # Normalize to prevent clipping\n",
    "        peak = np.max(np.abs(conversation_audio))\n",
    "        if peak > 0.95:\n",
    "            conversation_audio = conversation_audio * 0.95 / peak\n",
    "\n",
    "        # Save conversation audio\n",
    "        sf.write(output_path, conversation_audio, self.sr)\n",
    "\n",
    "        return total_duration\n",
    "\n",
    "    def generate_conversations(self, num_conversations: int = 5) -> List[Dict]:\n",
    "        \"\"\"Generate multiple conversation scenarios\"\"\"\n",
    "        metadata = []\n",
    "\n",
    "        print(f\"Starting generation of {num_conversations} conversations...\")\n",
    "        print(\n",
    "            f\"Configuration: {self.num_speakers} speakers, {self.turns_per_speaker} turns each\"\n",
    "        )\n",
    "\n",
    "        for conv_id in range(num_conversations):\n",
    "            try:\n",
    "                print(f\"\\n--- Conversation {conv_id + 1}/{num_conversations} ---\")\n",
    "\n",
    "                # Select speakers and files\n",
    "                print(\"  Selecting speakers and audio files...\")\n",
    "                audio_pool = self.select_speakers_and_files()\n",
    "                speaker_names = list(audio_pool.keys())\n",
    "                print(f\"  Selected speakers: {speaker_names}\")\n",
    "\n",
    "                # Create reference audios\n",
    "                print(\"  Creating reference audios...\")\n",
    "                ref_audios = self.create_reference_audios(audio_pool)\n",
    "\n",
    "                # Generate conversation sequence\n",
    "                print(\"  Generating conversation sequence...\")\n",
    "                turns = self.generate_conversation_sequence(audio_pool)\n",
    "                print(f\"  Created {len(turns)} speaker turns\")\n",
    "\n",
    "                # Create conversation audio\n",
    "                print(\"  Mixing conversation audio...\")\n",
    "                conv_filename = f\"conversation_{conv_id:03d}.wav\"\n",
    "                conv_path = self.output_dir / \"conversations\" / conv_filename\n",
    "                total_duration = self.create_conversation_audio(turns, conv_path)\n",
    "                print(f\"  Duration: {total_duration:.1f} seconds\")\n",
    "\n",
    "                # Create metadata\n",
    "                speakers_info = {}\n",
    "                for speaker_id in audio_pool.keys():\n",
    "                    speaker_turns = [\n",
    "                        turn for turn in turns if turn.speaker_id == speaker_id\n",
    "                    ]\n",
    "                    speakers_info[speaker_id] = {\n",
    "                        \"reference_audio\": ref_audios[speaker_id].as_posix(),\n",
    "                        \"turns\": [\n",
    "                            {\n",
    "                                \"start_time\": turn.start_time,\n",
    "                                \"end_time\": turn.end_time,\n",
    "                                \"transcript\": turn.transcript,\n",
    "                                \"overlap_with_previous\": turn.overlap_with_previous,\n",
    "                                \"audio_source\": turn.audio_path.as_posix(),\n",
    "                            }\n",
    "                            for turn in speaker_turns\n",
    "                        ],\n",
    "                    }\n",
    "\n",
    "                # Overlapping pairs for separation training\n",
    "                overlapping_pairs = []\n",
    "                for i in range(len(turns) - 1):\n",
    "                    current_turn = turns[i]\n",
    "                    next_turn = turns[i + 1]\n",
    "\n",
    "                    if (\n",
    "                        current_turn.speaker_id != next_turn.speaker_id\n",
    "                        and next_turn.overlap_with_previous > 0\n",
    "                    ):\n",
    "                        overlapping_pairs.append(\n",
    "                            {\n",
    "                                \"speaker_1\": current_turn.speaker_id,\n",
    "                                \"speaker_2\": next_turn.speaker_id,\n",
    "                                \"overlap_start\": next_turn.start_time,\n",
    "                                \"overlap_end\": current_turn.end_time,\n",
    "                                \"overlap_duration\": next_turn.overlap_with_previous,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                conversation_metadata = {\n",
    "                    \"conversation_id\": conv_id,\n",
    "                    \"conversation_audio\": conv_path.as_posix(),\n",
    "                    \"total_duration\": total_duration,\n",
    "                    \"num_speakers\": len(audio_pool),\n",
    "                    \"speakers\": speakers_info,\n",
    "                    \"overlapping_pairs\": overlapping_pairs,\n",
    "                    \"full_transcript\": \" \".join([turn.transcript for turn in turns]),\n",
    "                    \"speaker_sequence\": [turn.speaker_id for turn in turns],\n",
    "                }\n",
    "\n",
    "                metadata.append(conversation_metadata)\n",
    "                print(f\"  ✓ Conversation {conv_id + 1} completed!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Failed to generate conversation {conv_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Save metadata\n",
    "        metadata_path = self.output_dir / \"conversations_metadata.json\"\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\n=== Generation Complete ===\")\n",
    "        print(\n",
    "            f\"Successfully generated: {len(metadata)}/{num_conversations} conversations\"\n",
    "        )\n",
    "        print(f\"Metadata saved to: {metadata_path}\")\n",
    "\n",
    "        return metadata\n",
    "\n",
    "\n",
    "# Usage example\n",
    "def generate_conversation_dataset(\n",
    "    librispeech_root: str,\n",
    "    output_dir: str,\n",
    "    num_conversations: int = 5,\n",
    "    num_speakers: int = 3,\n",
    "    turns_per_speaker: int = 3,\n",
    "    clean_output_dir: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate conversation dataset\n",
    "\n",
    "    Args:\n",
    "        librispeech_root: Path to LibriSpeech dataset\n",
    "        output_dir: Output directory for conversations\n",
    "        num_conversations: Number of conversations to generate\n",
    "        num_speakers: Number of speakers per conversation\n",
    "        turns_per_speaker: Number of audio files per speaker\n",
    "        clean_output_dir: Whether to clean output directory first\n",
    "    \"\"\"\n",
    "\n",
    "    simulator = ConversationSimulator(\n",
    "        librispeech_root=librispeech_root,\n",
    "        output_dir=output_dir,\n",
    "        num_speakers=num_speakers,\n",
    "        turns_per_speaker=turns_per_speaker,\n",
    "        min_overlap_sec=0.5,\n",
    "        max_overlap_sec=2.0,\n",
    "        silence_between_speakers=0.2,\n",
    "        clean_output_dir=clean_output_dir,\n",
    "    )\n",
    "\n",
    "    metadata = simulator.generate_conversations(num_conversations)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n=== Conversation Dataset Summary ===\")\n",
    "    print(f\"Total conversations: {len(metadata)}\")\n",
    "    print(f\"Speakers per conversation: {num_speakers}\")\n",
    "    print(f\"Turns per speaker: {turns_per_speaker}\")\n",
    "\n",
    "    if metadata:\n",
    "        avg_duration = np.mean([conv[\"total_duration\"] for conv in metadata])\n",
    "        total_overlaps = sum(len(conv[\"overlapping_pairs\"]) for conv in metadata)\n",
    "        print(f\"Average conversation duration: {avg_duration:.1f} seconds\")\n",
    "        print(f\"Total overlapping segments: {total_overlaps}\")\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66412419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation of 5 conversations...\n",
      "Configuration: 4 speakers, 3 turns each\n",
      "\n",
      "--- Conversation 1/5 ---\n",
      "  Selecting speakers and audio files...\n",
      "Analyzing LibriSpeech speakers (one-time setup)...\n",
      "Found LibriSpeech subset: test-clean\n",
      "Searching in: ['LibriSpeech\\\\test-clean']\n",
      "Scanning directory: LibriSpeech\\test-clean\n",
      "Found 40 entries in LibriSpeech\\test-clean\n",
      "  Selected speakers: ['8463', '6829', '7021', '8224']\n",
      "  Creating reference audios...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_26136\\30653069.py:203: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  ref_file = max(files, key=lambda x: librosa.get_duration(filename=x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generating conversation sequence...\n",
      "  Created 12 speaker turns\n",
      "  Mixing conversation audio...\n",
      "  Duration: 84.4 seconds\n",
      "  ✓ Conversation 1 completed!\n",
      "\n",
      "--- Conversation 2/5 ---\n",
      "  Selecting speakers and audio files...\n",
      "  Selected speakers: ['7021', '7729', '1284', '4992']\n",
      "  Creating reference audios...\n",
      "  Generating conversation sequence...\n",
      "  Created 12 speaker turns\n",
      "  Mixing conversation audio...\n",
      "  Duration: 93.8 seconds\n",
      "  ✓ Conversation 2 completed!\n",
      "\n",
      "--- Conversation 3/5 ---\n",
      "  Selecting speakers and audio files...\n",
      "  Selected speakers: ['8230', '3575', '8224', '5142']\n",
      "  Creating reference audios...\n",
      "  Generating conversation sequence...\n",
      "  Created 12 speaker turns\n",
      "  Mixing conversation audio...\n",
      "  Duration: 90.9 seconds\n",
      "  ✓ Conversation 3 completed!\n",
      "\n",
      "--- Conversation 4/5 ---\n",
      "  Selecting speakers and audio files...\n",
      "  Selected speakers: ['3729', '4077', '1580', '7127']\n",
      "  Creating reference audios...\n",
      "  Generating conversation sequence...\n",
      "  Created 12 speaker turns\n",
      "  Mixing conversation audio...\n",
      "  Duration: 71.8 seconds\n",
      "  ✓ Conversation 4 completed!\n",
      "\n",
      "--- Conversation 5/5 ---\n",
      "  Selecting speakers and audio files...\n",
      "  Selected speakers: ['6930', '1188', '6829', '8555']\n",
      "  Creating reference audios...\n",
      "  Generating conversation sequence...\n",
      "  Created 12 speaker turns\n",
      "  Mixing conversation audio...\n",
      "  Duration: 87.3 seconds\n",
      "  ✓ Conversation 5 completed!\n",
      "\n",
      "=== Generation Complete ===\n",
      "Successfully generated: 5/5 conversations\n",
      "Metadata saved to: Pipeline_data\\Conv\\ver1\\conversations_metadata.json\n",
      "\n",
      "=== Conversation Dataset Summary ===\n",
      "Total conversations: 5\n",
      "Speakers per conversation: 4\n",
      "Turns per speaker: 3\n",
      "Average conversation duration: 85.6 seconds\n",
      "Total overlapping segments: 46\n"
     ]
    }
   ],
   "source": [
    "metadata = generate_conversation_dataset(\n",
    "    librispeech_root=\"LibriSpeech\",\n",
    "    output_dir=\"Pipeline_data/Conv/ver1\",\n",
    "    num_conversations=5,\n",
    "    num_speakers=4,\n",
    "    turns_per_speaker=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b53e78",
   "metadata": {},
   "source": [
    "## Seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e291c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from src.seperator import VoiceSeparator\n",
    "from utils.audio import Audio\n",
    "from utils.hparams import HParam\n",
    "from model.embedder import SpeechEmbedder\n",
    "\n",
    "\n",
    "class WorkingConversationPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline using your tested separator and ASR components\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, separator_config, asr_config, tuning_params=None):\n",
    "        self.separator_config = separator_config\n",
    "        self.asr_config = asr_config\n",
    "        self.sr = 16000\n",
    "        self.chunk_size = 3.0  # seconds\n",
    "        self.overlap = 0.05     # seconds\n",
    "        \n",
    "        # TUNABLE PARAMETERS \n",
    "        default_params = {\n",
    "            'similarity_threshold': 0.3,      # D-vector similarity threshold\n",
    "            'energy_threshold': 0.001,        # Audio energy threshold \n",
    "            'min_text_length': 2,             # Minimum text length\n",
    "            'confidence_threshold': 0.0,      # ASR confidence threshold (if available)\n",
    "            'time_gap_threshold': 1.0,        # Time gap for merging segments\n",
    "            'min_audio_length': 0.5,          # Minimum audio length for d-vector extraction\n",
    "            'debug_similarities': True,        # Show similarity scores for debugging\n",
    "            'contrast_threshold': 0.1,        # Minimum contrast between speakers\n",
    "        }\n",
    "        \n",
    "      \n",
    "        self.params = default_params.copy()\n",
    "        if tuning_params:\n",
    "            self.params.update(tuning_params)\n",
    "        \n",
    "        # print(f\"TUNING PARAMETERS:\")\n",
    "        # for key, value in self.params.items():\n",
    "        #     print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # Initialize your working models\n",
    "        self._init_separator()\n",
    "        self._init_asr()\n",
    "        self._init_embedder()\n",
    "        \n",
    "        # Cache for reference d-vectors\n",
    "        self.reference_dvecs = {}\n",
    "    \n",
    "    def _init_separator(self):\n",
    "        \"\"\"Initialize your working VoiceSeparator\"\"\"\n",
    "        self.separator = VoiceSeparator(\n",
    "            config_path=self.separator_config[\"config_path\"],\n",
    "            embedder_path=self.separator_config[\"embedder_path\"],\n",
    "            checkpoint_path=self.separator_config[\"checkpoint_path\"],\n",
    "            return_dvec=self.separator_config.get(\"return_dvec\", False),\n",
    "        )\n",
    "    \n",
    "    def _init_asr(self):\n",
    "        \"\"\"Initialize ASR using your working config\"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.stt_pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.asr_config[\"model\"],\n",
    "            device=device,\n",
    "            **self.asr_config.get(\"init_kwargs\", {}),\n",
    "        )\n",
    "    \n",
    "    def _init_embedder(self):\n",
    "        \"\"\"Initialize your working embedder for d-vector matching\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.hp = HParam(self.separator_config[\"config_path\"])\n",
    "        self.embedder = SpeechEmbedder(self.hp).to(device)\n",
    "        self.embedder.load_state_dict(\n",
    "            torch.load(self.separator_config[\"embedder_path\"], map_location=device)\n",
    "        )\n",
    "        self.embedder.eval()\n",
    "        self.audio_processor = Audio(self.hp)\n",
    "        self.device = device\n",
    "    \n",
    "    def extract_dvec(self, audio_path):\n",
    "        \"\"\"Extract d-vector using your working method\"\"\"\n",
    "        wav, _ = librosa.load(audio_path, sr=16000)\n",
    "        mel = self.audio_processor.get_mel(wav)\n",
    "        mel = torch.from_numpy(mel).float().to(self.device)\n",
    "        return self.embedder(mel).unsqueeze(0)  # [1, emb_dim]\n",
    "    \n",
    "    def get_reference_dvecs(self, speaker_references):\n",
    "        \"\"\"Cache reference d-vectors for all speakers\"\"\"\n",
    "        print(\" Extracting reference d-vectors...\")\n",
    "        for speaker_id, ref_path in speaker_references.items():\n",
    "            if speaker_id not in self.reference_dvecs:\n",
    "                self.reference_dvecs[speaker_id] = self.extract_dvec(ref_path)\n",
    "                print(f\"  ✓ {speaker_id}: {Path(ref_path).name}\")\n",
    "    \n",
    "    def chunk_audio(self, audio_data):\n",
    "        \"\"\"Split audio into overlapping chunks\"\"\"\n",
    "        chunk_samples = int(self.chunk_size * self.sr)\n",
    "        hop_samples = int((self.chunk_size - self.overlap) * self.sr)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(audio_data) - chunk_samples + 1, hop_samples):\n",
    "            chunk = audio_data[i:i + chunk_samples]\n",
    "            start_time = i / self.sr\n",
    "            end_time = (i + chunk_samples) / self.sr\n",
    "            \n",
    "            chunks.append({\n",
    "                'data': chunk,\n",
    "                'start_time': start_time,\n",
    "                'end_time': end_time,\n",
    "                'chunk_id': len(chunks)\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def separate_chunk_for_all_speakers(self, chunk, speaker_references, temp_dir=\"/tmp\"):\n",
    "        \"\"\"\n",
    "        Separate one chunk for all speakers using your working separator\n",
    "        \"\"\"\n",
    "        \n",
    "        chunk_path = f\"{temp_dir}/chunk_{chunk['chunk_id']}.wav\"\n",
    "        sf.write(chunk_path, chunk['data'], self.sr)\n",
    "        \n",
    "        separated_audios = {}\n",
    "        \n",
    "        for speaker_id, ref_path in speaker_references.items():\n",
    "            try:\n",
    "                \n",
    "                est_audio, dvec = self.separator.separate(\n",
    "                    reference_file=ref_path,\n",
    "                    mixed_file=chunk_path,\n",
    "                    out_dir=None, # don't let class save\n",
    "                )\n",
    "                \n",
    "                separated_audios[speaker_id] = est_audio\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Separation failed for {speaker_id} in chunk {chunk['chunk_id']}: {e}\")\n",
    "                separated_audios[speaker_id] = np.zeros_like(chunk['data'])\n",
    "        \n",
    "        # Clean up temp file\n",
    "        if os.path.exists(chunk_path):\n",
    "            os.remove(chunk_path)\n",
    "        \n",
    "        return separated_audios\n",
    "    \n",
    "    def diarize_using_dvector_matching(self, separated_audios, chunk_info, speaker_references):\n",
    "        \"\"\"\n",
    "        Diarize using your d-vector matching approach with tunable parameters\n",
    "        \"\"\"\n",
    "        chunk_start = chunk_info['start_time']\n",
    "        chunk_end = chunk_info['end_time']\n",
    "        chunk_id = chunk_info['chunk_id']\n",
    "        \n",
    "        # Extract d-vectors from separated audios\n",
    "        chunk_dvecs = {}\n",
    "        for speaker_id, separated_audio in separated_audios.items():\n",
    "            if len(separated_audio) > self.params['min_audio_length'] * self.sr:\n",
    "                try:\n",
    "                    mel = self.audio_processor.get_mel(separated_audio)\n",
    "                    mel_tensor = torch.from_numpy(mel).float().to(self.device)\n",
    "                    with torch.no_grad():\n",
    "                        emb = self.embedder(mel_tensor).unsqueeze(0)  # [1, emb_dim]\n",
    "                    chunk_dvecs[speaker_id] = emb\n",
    "                except Exception as e:\n",
    "                    if self.params['debug_similarities']:\n",
    "                        print(f\"     D-vector extraction failed for {speaker_id}: {e}\")\n",
    "                    chunk_dvecs[speaker_id] = None\n",
    "            else:\n",
    "                if self.params['debug_similarities']:\n",
    "                    print(f\"  {speaker_id}: Audio too short for d-vector extraction \"\n",
    "                          f\"({len(separated_audio)/self.sr:.2f}s < {self.params['min_audio_length']}s)\")\n",
    "                chunk_dvecs[speaker_id] = None\n",
    "        \n",
    "        # Compare with reference d-vectors to determine active speakers\n",
    "        active_speakers = {}\n",
    "        \n",
    "        if self.params['debug_similarities']:\n",
    "            print(f\"     SIMILARITY ANALYSIS:\")\n",
    "        \n",
    "        for speaker_id in speaker_references.keys():\n",
    "            if chunk_dvecs[speaker_id] is not None:\n",
    "                ref_dvec = self.reference_dvecs[speaker_id]\n",
    "                \n",
    "                # Compute similarity with correct speaker\n",
    "                sim_correct = torch.nn.functional.cosine_similarity(\n",
    "                    chunk_dvecs[speaker_id], ref_dvec\n",
    "                ).item()\n",
    "                \n",
    "                # Compute similarity with other speakers (for contrast)\n",
    "                sim_others = []\n",
    "                sim_details = {}\n",
    "                for other_speaker_id in speaker_references.keys():\n",
    "                    if other_speaker_id != speaker_id:\n",
    "                        other_ref_dvec = self.reference_dvecs[other_speaker_id]\n",
    "                        sim_other = torch.nn.functional.cosine_similarity(\n",
    "                            chunk_dvecs[speaker_id], other_ref_dvec\n",
    "                        ).item()\n",
    "                        sim_others.append(sim_other)\n",
    "                        sim_details[other_speaker_id] = sim_other\n",
    "                \n",
    "                max_sim_other = max(sim_others) if sim_others else 0.0\n",
    "                contrast_score = sim_correct - max_sim_other\n",
    "                \n",
    "                # Check energy level\n",
    "                energy = np.mean(separated_audios[speaker_id] ** 2)\n",
    "                \n",
    "                if self.params['debug_similarities']:\n",
    "                    print(f\"      {speaker_id}: sim_self={sim_correct:.3f}, \"\n",
    "                          f\"sim_others={max_sim_other:.3f}, contrast={contrast_score:.3f}, \"\n",
    "                          f\"energy={energy:.6f}\")\n",
    "                    for other_id, sim_val in sim_details.items():\n",
    "                        print(f\"        vs {other_id}: {sim_val:.3f}\")\n",
    "                \n",
    "                # Decision logic\n",
    "                passes_similarity = sim_correct > self.params['similarity_threshold']\n",
    "                passes_contrast = contrast_score > self.params['contrast_threshold']\n",
    "                passes_energy = energy > self.params['energy_threshold']\n",
    "                \n",
    "                if self.params['debug_similarities']:\n",
    "                    print(f\"        Tests: similarity={passes_similarity} \"\n",
    "                          f\"({sim_correct:.3f} > {self.params['similarity_threshold']}), \"\n",
    "                          f\"contrast={passes_contrast} \"\n",
    "                          f\"({contrast_score:.3f} > {self.params['contrast_threshold']}), \"\n",
    "                          f\"energy={passes_energy} \"\n",
    "                          f\"({energy:.6f} > {self.params['energy_threshold']})\")\n",
    "                \n",
    "                # Speaker is active if passes all tests\n",
    "                if passes_similarity and passes_contrast and passes_energy:\n",
    "                    active_speakers[speaker_id] = {\n",
    "                        'separated_audio': separated_audios[speaker_id],\n",
    "                        'similarity_score': sim_correct,\n",
    "                        'contrast_score': contrast_score,\n",
    "                        'energy_score': energy\n",
    "                    }\n",
    "                    print(f\"   [{chunk_start:5.1f}s] {speaker_id} ACTIVE \"\n",
    "                          f\"(sim: {sim_correct:.3f}, contrast: {contrast_score:.3f}, energy: {energy:.6f})\")\n",
    "                else:\n",
    "                    if self.params['debug_similarities']:\n",
    "                        print(f\"   [{chunk_start:5.1f}s] {speaker_id} INACTIVE \"\n",
    "                              f\"(failed: {[t for t, p in [('sim', passes_similarity), ('contrast', passes_contrast), ('energy', passes_energy)] if not p]})\")\n",
    "            else:\n",
    "                if self.params['debug_similarities']:\n",
    "                    print(f\"      {speaker_id}: No d-vector extracted\")\n",
    "        \n",
    "        return active_speakers\n",
    "    \n",
    "    def transcribe_active_speakers(self, active_speakers, chunk_info):\n",
    "        \"\"\"\n",
    "        Transcribe audio for active speakers using your ASR with tunable filtering\n",
    "        \"\"\"\n",
    "        transcriptions = []\n",
    "        \n",
    "        for speaker_id, speaker_data in active_speakers.items():\n",
    "            try:\n",
    "                separated_audio = speaker_data['separated_audio']\n",
    "                \n",
    "                # Check if audio has enough energy \n",
    "                energy = speaker_data['energy_score']\n",
    "                if energy < self.params['energy_threshold']:\n",
    "                    if self.params['debug_similarities']:\n",
    "                        print(f\"      {speaker_id}: Audio energy too low for transcription\")\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                result = self.stt_pipe(separated_audio)\n",
    "                text = result[\"text\"].strip()\n",
    "                \n",
    "                # Get confidence if available\n",
    "                confidence = result.get(\"confidence\", 1.0)  # Default to 1.0 if not available\n",
    "                \n",
    "                if self.params['debug_similarities']:\n",
    "                    print(f\"     {speaker_id} raw transcription: '{text}' (conf: {confidence:.3f})\")\n",
    "                \n",
    "                # Apply your text filtering with tunable parameters\n",
    "                if self._is_valid_transcription(text, confidence):\n",
    "                    transcriptions.append({\n",
    "                        'speaker_id': speaker_id,\n",
    "                        'start_time': chunk_info['start_time'],\n",
    "                        'end_time': chunk_info['end_time'],\n",
    "                        'text': text,\n",
    "                        'chunk_id': chunk_info['chunk_id'],\n",
    "                        'similarity_score': speaker_data['similarity_score'],\n",
    "                        'contrast_score': speaker_data['contrast_score'],\n",
    "                        'energy_score': energy,\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"     {speaker_id}: {text}\")\n",
    "                else:\n",
    "                    if self.params['debug_similarities']:\n",
    "                        print(f\"     {speaker_id}: Transcription filtered out\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\" Transcription failed for {speaker_id}: {e}\")\n",
    "        \n",
    "        return transcriptions\n",
    "    \n",
    "    def _is_valid_transcription(self, text, confidence=1.0):\n",
    "        \"\"\"Filter transcriptions using tunable parameters\"\"\"\n",
    "        if not text:\n",
    "            return False\n",
    "        \n",
    "        # Length check\n",
    "        if len(text) < self.params['min_text_length']:\n",
    "            if self.params['debug_similarities']:\n",
    "                print(f\"        Filter: Text too short ({len(text)} < {self.params['min_text_length']})\")\n",
    "            return False\n",
    "        \n",
    "        # Confidence check\n",
    "        if confidence < self.params['confidence_threshold']:\n",
    "            if self.params['debug_similarities']:\n",
    "                print(f\"        Filter: Confidence too low ({confidence:.3f} < {self.params['confidence_threshold']})\")\n",
    "            return False\n",
    "        \n",
    "        # Filter out common noise patterns\n",
    "        text_lower = text.lower().strip()\n",
    "        noise_patterns = [\n",
    "            r'^[\\s\\-\\.]+$',                    # Only punctuation/spaces\n",
    "            r'^(uh|um|ah|er|mm|hmm)[\\s\\-]*$',  # Filler words only\n",
    "            r'^\\[.*\\]$',                       # Bracket annotations\n",
    "            r'^[\\(\\)]+$',                      # Only parentheses\n",
    "        ]\n",
    "        \n",
    "        for pattern in noise_patterns:\n",
    "            if re.match(pattern, text_lower):\n",
    "                if self.params['debug_similarities']:\n",
    "                    print(f\"        Filter: Matches noise pattern '{pattern}'\")\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _normalize_text(self, text):\n",
    "        \"\"\"Your text normalization\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text) \n",
    "        return \" \".join(text.split())\n",
    "    \n",
    "    def process_conversation(self, conversation_metadata, output_dir):\n",
    "        \"\"\"\n",
    "        Main processing function using your working components\n",
    "        \"\"\"\n",
    "        conv_id = conversation_metadata['conversation_id']\n",
    "        audio_path = conversation_metadata['conversation_audio']\n",
    "        speakers = conversation_metadata['speakers']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" PROCESSING CONVERSATION {conv_id} WITH YOUR WORKING MODELS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\" Audio: {Path(audio_path).name}\")\n",
    "        print(f\" Speakers: {list(speakers.keys())}\")\n",
    "        \n",
    "        # Get speaker references\n",
    "        speaker_refs = {\n",
    "            speaker_id: info['reference_audio']\n",
    "            for speaker_id, info in speakers.items()\n",
    "        }\n",
    "        \n",
    "        # Extract reference d-vectors\n",
    "        self.get_reference_dvecs(speaker_refs)\n",
    "        \n",
    "        # Load conversation audio\n",
    "        audio_data, _ = librosa.load(audio_path, sr=self.sr)\n",
    "        print(f\"  Duration: {len(audio_data) / self.sr:.1f} seconds\")\n",
    "        \n",
    "        # Chunk audio\n",
    "        chunks = self.chunk_audio(audio_data)\n",
    "        print(f\" Created {len(chunks)} chunks ({self.chunk_size}s each, {self.overlap}s overlap)\")\n",
    "        \n",
    "        # Initialize speaker transcription lists\n",
    "        speaker_transcriptions = {speaker_id: [] for speaker_id in speaker_refs.keys()}\n",
    "        \n",
    "        # Process each chunk\n",
    "        print(f\"\\n Processing chunks...\")\n",
    "        for chunk in tqdm(chunks, desc=\"Processing\"):\n",
    "            print(f\"\\nChunk {chunk['chunk_id']} [{chunk['start_time']:.1f}s - {chunk['end_time']:.1f}s]:\")\n",
    "            \n",
    "            # Step 1: Separate for all speakers\n",
    "            separated_audios = self.separate_chunk_for_all_speakers(chunk, speaker_refs)\n",
    "            \n",
    "            # Step 2: Diarize using d-vector matching\n",
    "            active_speakers = self.diarize_using_dvector_matching(\n",
    "                separated_audios, chunk, speaker_refs\n",
    "            )\n",
    "            \n",
    "            # Step 3: Transcribe active speakers\n",
    "            if active_speakers:\n",
    "                transcriptions = self.transcribe_active_speakers(active_speakers, chunk)\n",
    "                \n",
    "                # Add to speaker lists\n",
    "                for trans in transcriptions:\n",
    "                    speaker_transcriptions[trans['speaker_id']].append(trans)\n",
    "            else:\n",
    "                print(f\"   No active speakers detected\")\n",
    "        \n",
    "        # Merge consecutive segments\n",
    "        print(f\"\\n Merging consecutive segments...\")\n",
    "        for speaker_id in speaker_transcriptions.keys():\n",
    "            original_count = len(speaker_transcriptions[speaker_id])\n",
    "            speaker_transcriptions[speaker_id] = self._merge_consecutive_segments(\n",
    "                speaker_transcriptions[speaker_id]\n",
    "            )\n",
    "            merged_count = len(speaker_transcriptions[speaker_id])\n",
    "            print(f\"  {speaker_id}: {original_count} → {merged_count} segments \"\n",
    "                  f\"(gap threshold: {self.params['time_gap_threshold']}s)\")\n",
    "        \n",
    "        # Save individual speaker JSONs\n",
    "        print(f\"\\n Saving speaker JSONs...\")\n",
    "        json_paths = {}\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for speaker_id, transcriptions in speaker_transcriptions.items():\n",
    "            json_path = self._save_speaker_json(\n",
    "                speaker_id, transcriptions, output_dir, conv_id\n",
    "            )\n",
    "            json_paths[speaker_id] = json_path\n",
    "            \n",
    "            total_duration = sum(\n",
    "                trans['end_time'] - trans['start_time'] \n",
    "                for trans in transcriptions\n",
    "            )\n",
    "            print(f\"   {speaker_id}: {len(transcriptions)} segments, {total_duration:.1f}s\")\n",
    "        \n",
    "        # Create summary\n",
    "        self._create_summary(speaker_transcriptions, output_dir, conv_id, conversation_metadata)\n",
    "        \n",
    "        print(f\"\\n PROCESSING COMPLETE!\")\n",
    "        print(f\" Output: {output_dir}\")\n",
    "        \n",
    "        return speaker_transcriptions, json_paths\n",
    "    \n",
    "    def _merge_consecutive_segments(self, transcriptions):\n",
    "        \"\"\"Merge consecutive segments from same speaker using tunable threshold\"\"\"\n",
    "        if not transcriptions:\n",
    "            return []\n",
    "        \n",
    "        # Sort by start time\n",
    "        transcriptions.sort(key=lambda x: x['start_time'])\n",
    "        \n",
    "        merged = []\n",
    "        current = transcriptions[0].copy()\n",
    "        \n",
    "        for next_trans in transcriptions[1:]:\n",
    "            # If segments are close in time, merge them\n",
    "            time_gap = next_trans['start_time'] - current['end_time']\n",
    "            \n",
    "            if time_gap <= self.params['time_gap_threshold']:\n",
    "                current['end_time'] = next_trans['end_time']\n",
    "                current['text'] = current['text'] + ' ' + next_trans['text']\n",
    "                current['similarity_score'] = max(\n",
    "                    current['similarity_score'], next_trans['similarity_score']\n",
    "                )\n",
    "                if self.params['debug_similarities']:\n",
    "                    print(f\"      Merged segments (gap: {time_gap:.2f}s <= {self.params['time_gap_threshold']}s)\")\n",
    "            else:\n",
    "                merged.append(current)\n",
    "                current = next_trans.copy()\n",
    "        \n",
    "        merged.append(current)\n",
    "        return merged\n",
    "    \n",
    "    def _save_speaker_json(self, speaker_id, transcriptions, output_dir, conversation_id):\n",
    "        \"\"\"Save transcriptions for one speaker to JSON\"\"\"\n",
    "       \n",
    "        segments = []\n",
    "        for trans in transcriptions:\n",
    "            segments.append({\n",
    "                'start_time': trans['start_time'],\n",
    "                'end_time': trans['end_time'],\n",
    "                'duration': trans['end_time'] - trans['start_time'],\n",
    "                'text': trans['text'],\n",
    "                'chunk_id': trans['chunk_id'],\n",
    "                'similarity_score': trans.get('similarity_score', 0.0),\n",
    "                'contrast_score': trans.get('contrast_score', 0.0)\n",
    "            })\n",
    "        \n",
    "        # Create speaker JSON\n",
    "        speaker_data = {\n",
    "            'speaker_id': speaker_id,\n",
    "            'conversation_id': conversation_id,\n",
    "            'total_segments': len(segments),\n",
    "            'total_duration': sum(seg['duration'] for seg in segments),\n",
    "            'segments': segments\n",
    "        }\n",
    "        \n",
    "        # Save JSON\n",
    "        json_path = output_dir / f\"{speaker_id}_transcription.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(speaker_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return json_path\n",
    "    \n",
    "    def _create_summary(self, speaker_transcriptions, output_dir, conv_id, conversation_metadata):\n",
    "        \"\"\"Create summary with timeline and ground truth comparison\"\"\"\n",
    "        # Create timeline\n",
    "        all_segments = []\n",
    "        for speaker_id, transcriptions in speaker_transcriptions.items():\n",
    "            for trans in transcriptions:\n",
    "                all_segments.append({\n",
    "                    'speaker_id': speaker_id,\n",
    "                    'start_time': trans['start_time'],\n",
    "                    'end_time': trans['end_time'],\n",
    "                    'text': trans['text'],\n",
    "                    'similarity_score': trans.get('similarity_score', 0.0)\n",
    "                })\n",
    "        \n",
    "        # Sort by time\n",
    "        all_segments.sort(key=lambda x: x['start_time'])\n",
    "        \n",
    "        # Create summary\n",
    "        summary_data = {\n",
    "            'conversation_id': conv_id,\n",
    "            'processing_method': 'working_separator_asr_pipeline',\n",
    "            'speakers': {\n",
    "                speaker_id: {\n",
    "                    'total_segments': len(transcriptions),\n",
    "                    'total_duration': sum(trans['end_time'] - trans['start_time'] for trans in transcriptions)\n",
    "                }\n",
    "                for speaker_id, transcriptions in speaker_transcriptions.items()\n",
    "            },\n",
    "            'timeline': all_segments,\n",
    "            'ground_truth': conversation_metadata['speakers']\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = output_dir / f\"conversation_{conv_id:03d}_summary.json\"\n",
    "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\" Summary saved: {summary_path}\")\n",
    "\n",
    "\n",
    "def process_conversation_with_your_models(conversations_metadata_path, \n",
    "                                        conversation_id=0,\n",
    "                                        separator_config=None,\n",
    "                                        asr_config=None,\n",
    "                                        tuning_params=None,\n",
    "                                        output_dir=\"working_transcriptions\"):\n",
    "    \"\"\"\n",
    "    Process conversation using your tested separator and ASR models with tunable parameters\n",
    "    \n",
    "    Args:\n",
    "        conversations_metadata_path: Path to conversation metadata\n",
    "        conversation_id: Which conversation to process  \n",
    "        separator_config: Voice separator configuration\n",
    "        asr_config: ASR configuration\n",
    "        tuning_params: Dict of tunable parameters for thresholds\n",
    "        output_dir: Output directory for JSON files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default configs based on your working code\n",
    "    if separator_config is None:\n",
    "        raise ValueError(\"Please provide a valid separator_config dictionary.\")\n",
    "    \n",
    "    if asr_config is None:\n",
    "        raise ValueError(\"Please provide a valid asr_config dictionary.\")\n",
    "    \n",
    "    # Load conversation metadata\n",
    "    with open(conversations_metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    if conversation_id >= len(metadata):\n",
    "        print(f\" Conversation {conversation_id} not found! Available: 0-{len(metadata)-1}\")\n",
    "        return None, None\n",
    "    \n",
    "    conversation = metadata[conversation_id]\n",
    "    \n",
    "    conv_output_dir = f\"{output_dir}/conversation_{conversation_id:03d}\"\n",
    "    \n",
    "    pipeline = WorkingConversationPipeline(separator_config, asr_config, tuning_params)\n",
    "    \n",
    "    transcriptions, json_paths = pipeline.process_conversation(conversation, conv_output_dir)\n",
    "    \n",
    "    return transcriptions, json_paths\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9964a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " PROCESSING CONVERSATION 1 WITH YOUR WORKING MODELS\n",
      "============================================================\n",
      " Audio: conversation_001.wav\n",
      " Speakers: ['7021', '7729', '1284', '4992']\n",
      " Extracting reference d-vectors...\n",
      "  ✓ 7021: 7021.wav\n",
      "  ✓ 7729: 7729.wav\n",
      "  ✓ 1284: 1284.wav\n",
      "  ✓ 4992: 4992.wav\n",
      "  Duration: 93.8 seconds\n",
      " Created 31 chunks (3.0s each, 0.05s overlap)\n",
      "\n",
      " Processing chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 0 [0.0s - 3.0s]:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\vsep\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [  0.0s] 4992 ACTIVE (sim: 0.747, contrast: 0.340, energy: 0.000756)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   3%|▎         | 1/31 [00:02<01:10,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4992: Nancy's Curly Chestnut Crop\n",
      "\n",
      "Chunk 1 [3.0s - 6.0s]:\n",
      "   [  3.0s] 4992 ACTIVE (sim: 0.681, contrast: 0.222, energy: 0.001619)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   6%|▋         | 2/31 [00:03<00:50,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4992: shown in the sun and olives thick black plates looked\n",
      "\n",
      "Chunk 2 [5.9s - 8.9s]:\n",
      "   [  5.9s] 7729 ACTIVE (sim: 0.705, contrast: 0.108, energy: 0.003128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|▉         | 3/31 [00:04<00:41,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: But the affair was magnified.\n",
      "\n",
      "Chunk 3 [8.8s - 11.8s]:\n",
      "   [  8.8s] 7729 ACTIVE (sim: 0.882, contrast: 0.404, energy: 0.002855)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|█▎        | 4/31 [00:06<00:37,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: as a crowning proof that the Free State\n",
      "\n",
      "Chunk 4 [11.8s - 14.8s]:\n",
      "   [ 11.8s] 7729 ACTIVE (sim: 0.833, contrast: 0.403, energy: 0.001997)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  16%|█▌        | 5/31 [00:07<00:34,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: and were insurrectionist and outlaws.\n",
      "\n",
      "Chunk 5 [14.8s - 17.8s]:\n",
      "   [ 14.8s] 7729 ACTIVE (sim: 0.868, contrast: 0.317, energy: 0.003991)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  19%|█▉        | 6/31 [00:08<00:32,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: In a few days an officer came with a\n",
      "\n",
      "Chunk 6 [17.7s - 20.7s]:\n",
      "   [ 17.7s] 7729 ACTIVE (sim: 0.892, contrast: 0.485, energy: 0.002513)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 7/31 [00:09<00:30,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: requisition from Governor Shannon and to the\n",
      "\n",
      "Chunk 7 [20.6s - 23.6s]:\n",
      "   [ 20.6s] 7729 ACTIVE (sim: 0.857, contrast: 0.365, energy: 0.002182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  26%|██▌       | 8/31 [00:10<00:28,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: prisoner by land to Westport, and afterward\n",
      "\n",
      "Chunk 8 [23.6s - 26.6s]:\n",
      "   [ 23.6s] 7021 ACTIVE (sim: 0.627, contrast: 0.085, energy: 0.002389)\n",
      "   [ 23.6s] 7729 ACTIVE (sim: 0.795, contrast: 0.325, energy: 0.001431)\n",
      "     7021: It truly is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  29%|██▉       | 9/31 [00:12<00:30,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: from there to Kansas City and Leavenworth.\n",
      "\n",
      "Chunk 9 [26.6s - 29.6s]:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [ 26.6s] 1284 ACTIVE (sim: 0.648, contrast: 0.270, energy: 0.000555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 10/31 [00:13<00:27,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1284: is asserted the\n",
      "\n",
      "Chunk 10 [29.5s - 32.5s]:\n",
      "   [ 29.5s] 1284 ACTIVE (sim: 0.708, contrast: 0.249, energy: 0.002833)\n",
      "   [ 29.5s] 4992 ACTIVE (sim: 0.696, contrast: 0.231, energy: 0.000769)\n",
      "     1284: was not so easy as you may suppose.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  35%|███▌      | 11/31 [00:15<00:28,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4992: And yet you might own her.\n",
      "\n",
      "Chunk 11 [32.5s - 35.5s]:\n",
      "   [ 32.5s] 4992 ACTIVE (sim: 0.938, contrast: 0.461, energy: 0.001968)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  39%|███▊      | 12/31 [00:16<00:26,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4992: her behavior has warranted them? Has it not been\n",
      "\n",
      "Chunk 12 [35.4s - 38.4s]:\n",
      "   [ 35.4s] 4992 ACTIVE (sim: 0.961, contrast: 0.459, energy: 0.001369)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  42%|████▏     | 13/31 [00:18<00:24,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4992: in this particular incoherent and unaccountable.\n",
      "\n",
      "Chunk 13 [38.4s - 41.4s]:\n",
      "   [ 38.4s] 1284 ACTIVE (sim: 0.936, contrast: 0.494, energy: 0.003997)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  45%|████▌     | 14/31 [00:19<00:23,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1284: prescribed by the civil and ecclesiastical powers of the Empire.\n",
      "\n",
      "Chunk 14 [41.3s - 44.3s]:\n",
      "   [ 41.3s] 1284 ACTIVE (sim: 0.930, contrast: 0.409, energy: 0.002537)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 15/31 [00:20<00:21,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1284: The Donatist still maintained in some proper\n",
      "\n",
      "Chunk 15 [44.2s - 47.2s]:\n",
      "   [ 44.2s] 1284 ACTIVE (sim: 0.928, contrast: 0.487, energy: 0.003048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  52%|█████▏    | 16/31 [00:21<00:19,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1284: particularly in the media, their superior numbers.\n",
      "\n",
      "Chunk 16 [47.2s - 50.2s]:\n",
      "   [ 47.2s] 1284 ACTIVE (sim: 0.942, contrast: 0.420, energy: 0.002828)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▍    | 17/31 [00:23<00:18,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1284: numbers and 400 bishops acknowledged the jurisdiction.\n",
      "\n",
      "Chunk 17 [50.1s - 53.1s]:\n",
      "   [ 50.1s] 7021 ACTIVE (sim: 0.740, contrast: 0.391, energy: 0.002037)\n",
      "   [ 50.1s] 1284 ACTIVE (sim: 0.842, contrast: 0.329, energy: 0.000918)\n",
      "     7021: But his mother hugged him\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  58%|█████▊    | 18/31 [00:24<00:18,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1284: restriction of their primate.\n",
      "\n",
      "Chunk 18 [53.1s - 56.1s]:\n",
      "   [ 53.1s] 7021 ACTIVE (sim: 0.856, contrast: 0.462, energy: 0.001570)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  61%|██████▏   | 19/31 [00:25<00:15,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7021: close. For instance,\n",
      "\n",
      "Chunk 19 [56.0s - 59.0s]:\n",
      "   [ 56.0s] 7021 ACTIVE (sim: 0.928, contrast: 0.477, energy: 0.004128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  65%|██████▍   | 20/31 [00:27<00:14,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7021: One day, the children had been playing a\n",
      "\n",
      "Chunk 20 [59.0s - 62.0s]:\n",
      "   [ 59.0s] 7021 ACTIVE (sim: 0.934, contrast: 0.547, energy: 0.003052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  68%|██████▊   | 21/31 [00:28<00:13,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7021: on the Piazza with blocks and other playthings.\n",
      "\n",
      "Chunk 21 [62.0s - 65.0s]:\n",
      "   [ 62.0s] 7021 ACTIVE (sim: 0.923, contrast: 0.565, energy: 0.004263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  71%|███████   | 22/31 [00:29<00:11,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7021: and finally had gone into the house.\n",
      "\n",
      "Chunk 22 [64.9s - 67.9s]:\n",
      "   [ 64.9s] 7021 ACTIVE (sim: 0.895, contrast: 0.512, energy: 0.006593)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  74%|███████▍  | 23/31 [00:31<00:10,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7021: leaving all the things on the floor of the piazza.\n",
      "\n",
      "Chunk 23 [67.8s - 70.8s]:\n",
      "   [ 67.8s] 7021 ACTIVE (sim: 0.930, contrast: 0.538, energy: 0.004258)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  77%|███████▋  | 24/31 [00:32<00:08,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7021: instead of putting them away in their place.\n",
      "\n",
      "Chunk 24 [70.8s - 73.8s]:\n",
      "   [ 70.8s] 7021 ACTIVE (sim: 0.902, contrast: 0.531, energy: 0.002554)\n",
      "   [ 70.8s] 7729 ACTIVE (sim: 0.799, contrast: 0.280, energy: 0.003097)\n",
      "     7021: as they ought to have done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  81%|████████  | 25/31 [00:34<00:08,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: The whole proceeding was so childish.\n",
      "\n",
      "Chunk 25 [73.8s - 76.8s]:\n",
      "   [ 73.8s] 7729 ACTIVE (sim: 0.937, contrast: 0.597, energy: 0.004132)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  84%|████████▍ | 26/31 [00:35<00:06,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: The miserable plot, so transparent.\n",
      "\n",
      "Chunk 26 [76.7s - 79.7s]:\n",
      "   [ 76.7s] 7729 ACTIVE (sim: 0.943, contrast: 0.533, energy: 0.003615)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  87%|████████▋ | 27/31 [00:36<00:05,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: the outrage so gross at spring disgust.\n",
      "\n",
      "Chunk 27 [79.7s - 82.7s]:\n",
      "   [ 79.7s] 7729 ACTIVE (sim: 0.928, contrast: 0.482, energy: 0.003854)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  90%|█████████ | 28/31 [00:37<00:03,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7729: to the better class of border ruffians who were witness\n",
      "\n",
      "Chunk 28 [82.6s - 85.6s]:\n",
      "   [ 82.6s] 7729 ACTIVE (sim: 0.771, contrast: 0.162, energy: 0.001145)\n",
      "   [ 82.6s] 4992 ACTIVE (sim: 0.742, contrast: 0.251, energy: 0.002557)\n",
      "     7729: and accessories.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  94%|█████████▎| 29/31 [00:39<00:02,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4992: What can you mean by that, Miss Woodward?\n",
      "\n",
      "Chunk 29 [85.5s - 88.5s]:\n",
      "   [ 85.5s] 7021 ACTIVE (sim: 0.658, contrast: 0.297, energy: 0.001460)\n",
      "   [ 85.5s] 4992 ACTIVE (sim: 0.749, contrast: 0.296, energy: 0.001925)\n",
      "     7021: You have come!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  97%|█████████▋| 30/31 [00:41<00:01,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4992: You talk mysteriously.\n",
      "\n",
      "Chunk 30 [88.5s - 91.5s]:\n",
      "   [ 88.5s] 7021 ACTIVE (sim: 0.816, contrast: 0.358, energy: 0.001007)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 31/31 [00:42<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     7021: Andella. Andella was the name of Gene's.\n",
      "\n",
      " Merging consecutive segments...\n",
      "  7021: 11 → 3 segments (gap threshold: 1.5s)\n",
      "  7729: 12 → 2 segments (gap threshold: 1.5s)\n",
      "  1284: 7 → 2 segments (gap threshold: 1.5s)\n",
      "  4992: 7 → 3 segments (gap threshold: 1.5s)\n",
      "\n",
      " Saving speaker JSONs...\n",
      "   7021: 3 segments, 32.6s\n",
      "   7729: 2 segments, 35.5s\n",
      "   1284: 2 segments, 20.7s\n",
      "   4992: 3 segments, 20.8s\n",
      " Summary saved: pipeline_data\\Conv\\ver1\\seperated_transcriptions\\conversation_001\\conversation_001_summary.json\n",
      "\n",
      " PROCESSING COMPLETE!\n",
      " Output: pipeline_data\\Conv\\ver1\\seperated_transcriptions\\conversation_001\n",
      "\n",
      " SUCCESS! Generated speaker JSONs:\n",
      " 7021: pipeline_data\\Conv\\ver1\\seperated_transcriptions\\conversation_001\\7021_transcription.json (3 segments)\n",
      " 7729: pipeline_data\\Conv\\ver1\\seperated_transcriptions\\conversation_001\\7729_transcription.json (2 segments)\n",
      " 1284: pipeline_data\\Conv\\ver1\\seperated_transcriptions\\conversation_001\\1284_transcription.json (2 segments)\n",
      " 4992: pipeline_data\\Conv\\ver1\\seperated_transcriptions\\conversation_001\\4992_transcription.json (3 segments)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "separator_config = {\n",
    "    \"config_path\": \"config/inference.yaml\",\n",
    "    \"embedder_path\": \"ckpt/embedder.pt\",\n",
    "    \"checkpoint_path\": \"ckpt/seperator_best_checkpoint.pt\",\n",
    "    \"return_dvec\": False,\n",
    "}\n",
    "\n",
    "asr_config = {\n",
    "    \"model\": \"ckpt/whisper-small\", \n",
    "    \"init_kwargs\": {},\n",
    "}\n",
    "\n",
    "# TUNABLE PARAMETERS\n",
    "tuning_params = {\n",
    "    \"similarity_threshold\": 0.2,  # Lower = more permissive (try 0.1-0.5)\n",
    "    \"energy_threshold\": 0.0005,  # Lower = more permissive (try 0.0001-0.01)\n",
    "    \"min_text_length\": 1,  # Lower = more permissive (try 1-5)\n",
    "    \"confidence_threshold\": 0.0,  # ASR confidence threshold (try 0.0-0.5)\n",
    "    \"time_gap_threshold\": 1.5,  # Time gap for merging (try 0.5-3.0)\n",
    "    \"min_audio_length\": 0.3,  # Minimum audio for d-vector (try 0.1-1.0)\n",
    "    \"debug_similarities\": False,  # Show detailed similarity scores\n",
    "    \"contrast_threshold\": 0.05,  # Lower = more permissive (try 0.0-0.3)\n",
    "}\n",
    "\n",
    "transcriptions, json_paths = process_conversation_with_your_models(\n",
    "    conversations_metadata_path=\"Pipeline_data/Conv/ver1/conversations_metadata.json\",\n",
    "    conversation_id=1,\n",
    "    separator_config=separator_config,\n",
    "    asr_config=asr_config,\n",
    "    tuning_params=tuning_params,  # Pass tuning parameters\n",
    "    output_dir=\"pipeline_data/Conv/ver1/seperated_transcriptions\",\n",
    ")\n",
    "\n",
    "if transcriptions:\n",
    "    print(f\"\\n SUCCESS! Generated speaker JSONs:\")\n",
    "    for speaker_id, json_path in json_paths.items():\n",
    "        count = len(transcriptions[speaker_id])\n",
    "        print(f\" {speaker_id}: {json_path} ({count} segments)\")\n",
    "else:\n",
    "    print(f\"\\n No transcriptions generated. Try adjusting tuning_params:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "406fe626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08ee89",
   "metadata": {},
   "source": [
    "## Evaluvator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9246420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from jiwer import wer, mer, wil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "class ConversationPipelineEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluator for the WorkingConversationPipeline\n",
    "    Assesses both transcription quality and speaker diarization accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "        self.speaker_results = {}\n",
    "        self.conversation_results = {}\n",
    "        \n",
    "    def _normalize_text(self, text):\n",
    "        \"\"\"Normalize text for comparison\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "        return \" \".join(text.split())\n",
    "    \n",
    "    def _load_ground_truth(self):\n",
    "        \"\"\"Load ground truth conversation metadata\"\"\"\n",
    "        with open(self.config[\"ground_truth_path\"], \"r\") as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def _load_pipeline_output(self, conversation_id, output_dir):\n",
    "        \"\"\"Load pipeline output for a specific conversation\"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "        \n",
    "        # Load summary\n",
    "        summary_path = output_dir / f\"conversation_{conversation_id:03d}_summary.json\"\n",
    "        if not summary_path.exists():\n",
    "            return None, {}\n",
    "        \n",
    "        with open(summary_path, \"r\") as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        # Load individual speaker JSONs\n",
    "        speaker_data = {}\n",
    "        for speaker_id in summary.get(\"speakers\", {}).keys():\n",
    "            speaker_path = output_dir / f\"{speaker_id}_transcription.json\"\n",
    "            if speaker_path.exists():\n",
    "                with open(speaker_path, \"r\") as f:\n",
    "                    speaker_data[speaker_id] = json.load(f)\n",
    "        \n",
    "        return summary, speaker_data\n",
    "    \n",
    "    def _extract_ground_truth_segments(self, gt_conversation):\n",
    "        \"\"\"Extract ground truth segments by speaker\"\"\"\n",
    "        gt_segments = {}\n",
    "        \n",
    "        # Check if ground truth has detailed segments or turns structure\n",
    "        if \"detailed_segments\" in gt_conversation:\n",
    "            # Use detailed ground truth segments (original format)\n",
    "            for segment in gt_conversation[\"detailed_segments\"]:\n",
    "                speaker_id = segment[\"speaker_id\"]\n",
    "                if speaker_id not in gt_segments:\n",
    "                    gt_segments[speaker_id] = []\n",
    "                gt_segments[speaker_id].append({\n",
    "                    \"start_time\": segment[\"start_time\"],\n",
    "                    \"end_time\": segment[\"end_time\"],\n",
    "                    \"text\": segment[\"text\"]\n",
    "                })\n",
    "        elif \"speakers\" in gt_conversation:\n",
    "            # Use speakers with turns structure (your format)\n",
    "            for speaker_id, speaker_info in gt_conversation[\"speakers\"].items():\n",
    "                if \"turns\" in speaker_info:\n",
    "                    # Extract segments from turns\n",
    "                    gt_segments[speaker_id] = []\n",
    "                    for turn in speaker_info[\"turns\"]:\n",
    "                        gt_segments[speaker_id].append({\n",
    "                            \"start_time\": turn[\"start_time\"],\n",
    "                            \"end_time\": turn[\"end_time\"], \n",
    "                            \"text\": turn[\"transcript\"]\n",
    "                        })\n",
    "                elif \"full_transcript\" in speaker_info:\n",
    "                    # Use overall speaker transcripts (create single segment per speaker)\n",
    "                    gt_segments[speaker_id] = [{\n",
    "                        \"start_time\": 0,\n",
    "                        \"end_time\": gt_conversation.get(\"total_duration\", 300),\n",
    "                        \"text\": speaker_info[\"full_transcript\"]\n",
    "                    }]\n",
    "        \n",
    "        return gt_segments\n",
    "    \n",
    "    def _calculate_speaker_wer(self, predicted_segments, ground_truth_segments):\n",
    "        \"\"\"Calculate WER for a specific speaker\"\"\"\n",
    "        if not predicted_segments or not ground_truth_segments:\n",
    "            return {\n",
    "                \"wer\": 1.0,  # 100% error if no segments\n",
    "                \"mer\": 1.0,\n",
    "                \"wil\": 1.0,\n",
    "                \"predicted_text\": \"\",\n",
    "                \"ground_truth_text\": \"\",\n",
    "                \"segment_count\": 0\n",
    "            }\n",
    "        \n",
    "        # Combine all segments into full text\n",
    "        pred_text = \" \".join([seg[\"text\"] for seg in predicted_segments])\n",
    "        gt_text = \" \".join([seg[\"text\"] for seg in ground_truth_segments])\n",
    "        \n",
    "        # Normalize texts\n",
    "        pred_norm = self._normalize_text(pred_text)\n",
    "        gt_norm = self._normalize_text(gt_text)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if not gt_norm:\n",
    "            return {\n",
    "                \"wer\": 1.0 if pred_norm else 0.0,\n",
    "                \"mer\": 1.0 if pred_norm else 0.0,\n",
    "                \"wil\": 1.0 if pred_norm else 0.0,\n",
    "                \"predicted_text\": pred_text,\n",
    "                \"ground_truth_text\": gt_text,\n",
    "                \"segment_count\": len(predicted_segments)\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            speaker_wer = wer(gt_norm, pred_norm)\n",
    "            speaker_mer = mer(gt_norm, pred_norm)\n",
    "            speaker_wil = wil(gt_norm, pred_norm)\n",
    "        except:\n",
    "            # Handle edge cases\n",
    "            speaker_wer = 1.0\n",
    "            speaker_mer = 1.0\n",
    "            speaker_wil = 1.0\n",
    "        \n",
    "        return {\n",
    "            \"wer\": speaker_wer,\n",
    "            \"mer\": speaker_mer,\n",
    "            \"wil\": speaker_wil,\n",
    "            \"predicted_text\": pred_text,\n",
    "            \"ground_truth_text\": gt_text,\n",
    "            \"normalized_predicted\": pred_norm,\n",
    "            \"normalized_ground_truth\": gt_norm,\n",
    "            \"segment_count\": len(predicted_segments)\n",
    "        }\n",
    "    \n",
    "    def _calculate_diarization_metrics(self, predicted_timeline, ground_truth_segments):\n",
    "        \"\"\"Calculate diarization accuracy metrics\"\"\"\n",
    "\n",
    "        total_time = 0\n",
    "        correct_time = 0\n",
    "        \n",
    "        sampling_rate = 0.1\n",
    "        max_time = max([seg[\"end_time\"] for segments in ground_truth_segments.values() \n",
    "                       for seg in segments], default=300)\n",
    "        \n",
    "        for t in np.arange(0, max_time, sampling_rate):\n",
    "            # Find ground truth speaker at time t\n",
    "            gt_speaker = None\n",
    "            for speaker_id, segments in ground_truth_segments.items():\n",
    "                for seg in segments:\n",
    "                    if seg[\"start_time\"] <= t <= seg[\"end_time\"]:\n",
    "                        gt_speaker = speaker_id\n",
    "                        break\n",
    "                if gt_speaker:\n",
    "                    break\n",
    "            \n",
    "            # Find predicted speaker at time t\n",
    "            pred_speaker = None\n",
    "            for seg in predicted_timeline:\n",
    "                if seg[\"start_time\"] <= t <= seg[\"end_time\"]:\n",
    "                    pred_speaker = seg[\"speaker_id\"]\n",
    "                    break\n",
    "            \n",
    "            if gt_speaker is not None:  # Only count time where there should be speech\n",
    "                total_time += sampling_rate\n",
    "                if gt_speaker == pred_speaker:\n",
    "                    correct_time += sampling_rate\n",
    "        \n",
    "        diarization_accuracy = correct_time / total_time if total_time > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"diarization_accuracy\": diarization_accuracy,\n",
    "            \"total_evaluated_time\": total_time,\n",
    "            \"correctly_assigned_time\": correct_time\n",
    "        }\n",
    "    \n",
    "    def _calculate_speaker_coverage(self, predicted_speakers, ground_truth_speakers):\n",
    "        \"\"\"Calculate how well we detected each speaker\"\"\"\n",
    "        coverage = {}\n",
    "        \n",
    "        for gt_speaker in ground_truth_speakers:\n",
    "            if gt_speaker in predicted_speakers:\n",
    "                pred_duration = sum(seg[\"duration\"] for seg in predicted_speakers[gt_speaker])\n",
    "                # Rough estimate of ground truth duration\n",
    "                gt_duration = 60  # Default assumption, should be calculated from GT\n",
    "                coverage[gt_speaker] = {\n",
    "                    \"detected\": True,\n",
    "                    \"predicted_duration\": pred_duration,\n",
    "                    \"coverage_ratio\": min(pred_duration / gt_duration, 1.0)\n",
    "                }\n",
    "            else:\n",
    "                coverage[gt_speaker] = {\n",
    "                    \"detected\": False,\n",
    "                    \"predicted_duration\": 0,\n",
    "                    \"coverage_ratio\": 0.0\n",
    "                }\n",
    "        \n",
    "        # Check for false positive speakers\n",
    "        for pred_speaker in predicted_speakers:\n",
    "            if pred_speaker not in ground_truth_speakers:\n",
    "                coverage[f\"{pred_speaker}_FP\"] = {\n",
    "                    \"false_positive\": True,\n",
    "                    \"predicted_duration\": sum(seg[\"duration\"] for seg in predicted_speakers[pred_speaker])\n",
    "                }\n",
    "        \n",
    "        return coverage\n",
    "    \n",
    "    def evaluate_conversation(self, conversation_id, output_dir):\n",
    "        \"\"\"Evaluate a single conversation\"\"\"\n",
    "        print(f\"\\n Evaluating Conversation {conversation_id}\")\n",
    "        \n",
    "        # Load ground truth\n",
    "        ground_truth = self._load_ground_truth()\n",
    "        if conversation_id >= len(ground_truth):\n",
    "            print(f\" Conversation {conversation_id} not found in ground truth\")\n",
    "            return None\n",
    "        \n",
    "        gt_conversation = ground_truth[conversation_id]\n",
    "        print(f\"   Ground truth loaded: {len(gt_conversation.get('speakers', {}))} speakers\")\n",
    "        \n",
    "        # Load pipeline output\n",
    "        summary, speaker_data = self._load_pipeline_output(conversation_id, output_dir)\n",
    "        if summary is None:\n",
    "            print(f\" Pipeline output not found for conversation {conversation_id}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"   Pipeline output loaded: {len(speaker_data)} speaker files\")\n",
    "        \n",
    "        # Extract ground truth segments\n",
    "        gt_segments = self._extract_ground_truth_segments(gt_conversation)\n",
    "        print(f\"   Ground truth segments extracted for speakers: {list(gt_segments.keys())}\")\n",
    "        \n",
    "        # Debug: Show segment counts\n",
    "        for speaker_id, segments in gt_segments.items():\n",
    "            total_duration = sum(seg[\"end_time\"] - seg[\"start_time\"] for seg in segments)\n",
    "            print(f\"    {speaker_id}: {len(segments)} segments, {total_duration:.1f}s total\")\n",
    "        \n",
    "        # Evaluate each speaker\n",
    "        speaker_metrics = {}\n",
    "        for speaker_id in gt_segments.keys():\n",
    "            print(f\"   Evaluating {speaker_id}...\")\n",
    "            \n",
    "            # Get predicted segments for this speaker\n",
    "            predicted_segments = []\n",
    "            if speaker_id in speaker_data:\n",
    "                predicted_segments = speaker_data[speaker_id][\"segments\"]\n",
    "                print(f\"    Found {len(predicted_segments)} predicted segments\")\n",
    "            else:\n",
    "                print(f\"    No predicted segments found for {speaker_id}\")\n",
    "            \n",
    "            # Calculate WER for this speaker\n",
    "            speaker_wer_result = self._calculate_speaker_wer(\n",
    "                predicted_segments, gt_segments[speaker_id]\n",
    "            )\n",
    "            \n",
    "            speaker_metrics[speaker_id] = speaker_wer_result\n",
    "            \n",
    "            print(f\"    WER: {speaker_wer_result['wer']:.2%}\")\n",
    "            print(f\"    Segments: {speaker_wer_result['segment_count']}\")\n",
    "        \n",
    "        # Calculate diarization metrics\n",
    "        predicted_timeline = summary.get(\"timeline\", [])\n",
    "        print(f\"   Timeline has {len(predicted_timeline)} segments\")\n",
    "        diarization_metrics = self._calculate_diarization_metrics(predicted_timeline, gt_segments)\n",
    "        \n",
    "        # Calculate speaker coverage\n",
    "        predicted_speakers = {sid: data[\"segments\"] for sid, data in speaker_data.items()}\n",
    "        coverage_metrics = self._calculate_speaker_coverage(predicted_speakers, gt_segments.keys())\n",
    "        \n",
    "        # Compile conversation result\n",
    "        conversation_result = {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"speaker_metrics\": speaker_metrics,\n",
    "            \"diarization_metrics\": diarization_metrics,\n",
    "            \"coverage_metrics\": coverage_metrics,\n",
    "            \"overall_wer\": float(np.mean([m[\"wer\"] for m in speaker_metrics.values()]) if speaker_metrics else 1.0),\n",
    "            \"total_predicted_speakers\": len(speaker_data),\n",
    "            \"total_ground_truth_speakers\": len(gt_segments),\n",
    "            \"predicted_segments_total\": sum(len(data[\"segments\"]) for data in speaker_data.values())\n",
    "        }\n",
    "        \n",
    "        self.conversation_results[conversation_id] = conversation_result\n",
    "        \n",
    "        print(f\"   Overall WER: {conversation_result['overall_wer']:.2%}\")\n",
    "        print(f\"   Diarization Accuracy: {diarization_metrics['diarization_accuracy']:.2%}\")\n",
    "        print(f\"   Speaker Detection: {len(speaker_data)}/{len(gt_segments)}\")\n",
    "        \n",
    "        return conversation_result\n",
    "    \n",
    "    def evaluate_all_conversations(self, output_base_dir):\n",
    "        \"\"\"Evaluate all conversations in the output directory\"\"\"\n",
    "        print(f\" Starting comprehensive evaluation...\")\n",
    "        \n",
    "        # Find all conversation output directories\n",
    "        output_base_path = Path(output_base_dir)\n",
    "        conversation_dirs = list(output_base_path.glob(\"conversation_*\"))\n",
    "        \n",
    "        if not conversation_dirs:\n",
    "            print(f\" No conversation directories found in {output_base_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Extract conversation IDs\n",
    "        conversation_ids = []\n",
    "        for dir_path in conversation_dirs:\n",
    "            try:\n",
    "                conv_id = int(dir_path.name.split(\"_\")[1])\n",
    "                conversation_ids.append(conv_id)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        conversation_ids.sort()\n",
    "        print(f\" Found {len(conversation_ids)} conversations to evaluate\")\n",
    "        \n",
    "        # Evaluate each conversation\n",
    "        all_results = []\n",
    "        for conv_id in tqdm(conversation_ids, desc=\"Evaluating conversations\"):\n",
    "            conv_dir = output_base_path / f\"conversation_{conv_id:03d}\"\n",
    "            result = self.evaluate_conversation(conv_id, conv_dir)\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "        \n",
    "        # Calculate overall statistics\n",
    "        self._calculate_overall_statistics(all_results)\n",
    "        \n",
    "        # Save results\n",
    "        self._save_evaluation_results(all_results)\n",
    "        \n",
    "        # Generate report\n",
    "        self._generate_evaluation_report(all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def _calculate_overall_statistics(self, all_results):\n",
    "        \"\"\"Calculate overall statistics across all conversations\"\"\"\n",
    "        if not all_results:\n",
    "            return\n",
    "        \n",
    "        # WER statistics\n",
    "        all_wers = []\n",
    "        speaker_wers = {}\n",
    "        \n",
    "        for result in all_results:\n",
    "            all_wers.append(result[\"overall_wer\"])\n",
    "            \n",
    "            for speaker_id, metrics in result[\"speaker_metrics\"].items():\n",
    "                if speaker_id not in speaker_wers:\n",
    "                    speaker_wers[speaker_id] = []\n",
    "                speaker_wers[speaker_id].append(metrics[\"wer\"])\n",
    "        \n",
    "        # Diarization statistics\n",
    "        diarization_scores = [r[\"diarization_metrics\"][\"diarization_accuracy\"] for r in all_results]\n",
    "        \n",
    "        # Speaker detection statistics\n",
    "        detection_rates = []\n",
    "        for result in all_results:\n",
    "            detected = result[\"total_predicted_speakers\"]\n",
    "            total = result[\"total_ground_truth_speakers\"]\n",
    "            detection_rates.append(detected / total if total > 0 else 0)\n",
    "        \n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        self.overall_stats = {\n",
    "            \"wer\": {\n",
    "                \"mean\": float(np.mean(all_wers)),\n",
    "                \"std\": float(np.std(all_wers)),\n",
    "                \"min\": float(np.min(all_wers)),\n",
    "                \"max\": float(np.max(all_wers)),\n",
    "                \"median\": float(np.median(all_wers))\n",
    "            },\n",
    "            \"diarization_accuracy\": {\n",
    "                \"mean\": float(np.mean(diarization_scores)),\n",
    "                \"std\": float(np.std(diarization_scores)), \n",
    "                \"min\": float(np.min(diarization_scores)),\n",
    "                \"max\": float(np.max(diarization_scores)),\n",
    "                \"median\": float(np.median(diarization_scores))\n",
    "            },\n",
    "            \"speaker_detection_rate\": {\n",
    "                \"mean\": float(np.mean(detection_rates)),\n",
    "                \"std\": float(np.std(detection_rates)),\n",
    "                \"min\": float(np.min(detection_rates)),\n",
    "                \"max\": float(np.max(detection_rates)),\n",
    "                \"median\": float(np.median(detection_rates))\n",
    "            },\n",
    "            \"per_speaker_wer\": {\n",
    "                speaker_id: {\n",
    "                    \"mean\": float(np.mean(wers)),\n",
    "                    \"count\": int(len(wers))\n",
    "                } for speaker_id, wers in speaker_wers.items()\n",
    "            },\n",
    "            \"total_conversations\": int(len(all_results))\n",
    "        }\n",
    "    \n",
    "    def _convert_numpy_types(self, obj):\n",
    "        \"\"\"Recursively convert numpy types to Python native types for JSON serialization\"\"\"\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_numpy_types(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    def _save_evaluation_results(self, all_results):\n",
    "        \"\"\"Save detailed evaluation results to JSON\"\"\"\n",
    "        output_dir = Path(self.config[\"output_dir\"])\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Convert numpy types to Python types\n",
    "        safe_results = self._convert_numpy_types({\n",
    "            \"config\": self.config,\n",
    "            \"overall_statistics\": self.overall_stats,\n",
    "            \"conversation_results\": all_results\n",
    "        })\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_path = output_dir / \"detailed_evaluation_results.json\"\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(safe_results, f, indent=2)\n",
    "        \n",
    "        print(f\" Detailed results saved to: {results_path}\")\n",
    "    \n",
    "    def _generate_evaluation_report(self, all_results):\n",
    "        \"\"\"Generate a human-readable evaluation report\"\"\"\n",
    "        output_dir = Path(self.config[\"output_dir\"])\n",
    "        report_path = output_dir / \"evaluation_report.txt\"\n",
    "        \n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"CONVERSATION PIPELINE EVALUATION REPORT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            # Overall Statistics\n",
    "            f.write(\"OVERALL PERFORMANCE METRICS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            f.write(f\"Total Conversations Evaluated: {self.overall_stats['total_conversations']}\\n\\n\")\n",
    "            \n",
    "            # WER Results\n",
    "            wer_stats = self.overall_stats['wer']\n",
    "            f.write(\"WORD ERROR RATE (WER)\\n\")\n",
    "            f.write(f\"  Mean WER: {wer_stats['mean']:.2%}\\n\")\n",
    "            f.write(f\"  Median WER: {wer_stats['median']:.2%}\\n\")\n",
    "            f.write(f\"  Best WER: {wer_stats['min']:.2%}\\n\")\n",
    "            f.write(f\"  Worst WER: {wer_stats['max']:.2%}\\n\")\n",
    "            f.write(f\"  Std Dev: ±{wer_stats['std']:.2%}\\n\\n\")\n",
    "            \n",
    "            # Diarization Results\n",
    "            diar_stats = self.overall_stats['diarization_accuracy']\n",
    "            f.write(\"SPEAKER DIARIZATION ACCURACY\\n\")\n",
    "            f.write(f\"  Mean Accuracy: {diar_stats['mean']:.2%}\\n\")\n",
    "            f.write(f\"  Median Accuracy: {diar_stats['median']:.2%}\\n\")\n",
    "            f.write(f\"  Best Accuracy: {diar_stats['max']:.2%}\\n\")\n",
    "            f.write(f\"  Worst Accuracy: {diar_stats['min']:.2%}\\n\")\n",
    "            f.write(f\"  Std Dev: ±{diar_stats['std']:.2%}\\n\\n\")\n",
    "            \n",
    "            # Speaker Detection\n",
    "            detect_stats = self.overall_stats['speaker_detection_rate']\n",
    "            f.write(\"SPEAKER DETECTION RATE\\n\")\n",
    "            f.write(f\"  Mean Detection Rate: {detect_stats['mean']:.2%}\\n\")\n",
    "            f.write(f\"  Median Detection Rate: {detect_stats['median']:.2%}\\n\")\n",
    "            f.write(f\"  Perfect Detection Rate: {detect_stats['max']:.2%}\\n\")\n",
    "            f.write(f\"  Worst Detection Rate: {detect_stats['min']:.2%}\\n\\n\")\n",
    "            \n",
    "            # Per-Speaker Analysis\n",
    "            f.write(\"PER-SPEAKER WER ANALYSIS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            for speaker_id, stats in self.overall_stats['per_speaker_wer'].items():\n",
    "                f.write(f\"  {speaker_id}: {stats['mean']:.2%} WER ({stats['count']} conversations)\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # Conversation-by-Conversation Results\n",
    "            f.write(\"CONVERSATION-BY-CONVERSATION RESULTS\\n\")\n",
    "            f.write(\"-\"*40 + \"\\n\")\n",
    "            for result in all_results:\n",
    "                conv_id = result['conversation_id']\n",
    "                overall_wer = result['overall_wer']\n",
    "                diar_acc = result['diarization_metrics']['diarization_accuracy']\n",
    "                speakers_detected = result['total_predicted_speakers']\n",
    "                speakers_total = result['total_ground_truth_speakers']\n",
    "                \n",
    "                f.write(f\"Conversation {conv_id:03d}: WER={overall_wer:.2%}, \"\n",
    "                       f\"Diarization={diar_acc:.2%}, Speakers={speakers_detected}/{speakers_total}\\n\")\n",
    "        \n",
    "        print(f\" Evaluation report saved to: {report_path}\")\n",
    "        \n",
    "        # Print summary to console\n",
    "        print(f\"\\n EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Mean WER: {wer_stats['mean']:.2%}\")\n",
    "        print(f\"Mean Diarization Accuracy: {diar_stats['mean']:.2%}\")\n",
    "        print(f\"Mean Speaker Detection Rate: {detect_stats['mean']:.2%}\")\n",
    "        print(f\"Total Conversations: {self.overall_stats['total_conversations']}\")\n",
    "\n",
    "\n",
    "def evaluate_pipeline_results(\n",
    "    ground_truth_path,\n",
    "    pipeline_output_dir,\n",
    "    output_dir=\"evaluation_results\",\n",
    "    conversation_ids=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate pipeline results against ground truth\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_path: Path to conversation metadata JSON (ground truth)\n",
    "        pipeline_output_dir: Directory containing pipeline outputs\n",
    "        output_dir: Directory to save evaluation results\n",
    "        conversation_ids: List of specific conversation IDs to evaluate (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"ground_truth_path\": ground_truth_path,\n",
    "        \"pipeline_output_dir\": pipeline_output_dir,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"conversation_ids\": conversation_ids\n",
    "    }\n",
    "    \n",
    "    evaluator = ConversationPipelineEvaluator(config)\n",
    "    \n",
    "    if conversation_ids is None:\n",
    "        # Evaluate all conversations\n",
    "        results = evaluator.evaluate_all_conversations(pipeline_output_dir)\n",
    "    else:\n",
    "        # Evaluate specific conversations\n",
    "        results = []\n",
    "        for conv_id in conversation_ids:\n",
    "            conv_dir = Path(pipeline_output_dir) / f\"conversation_{conv_id:03d}\"\n",
    "            result = evaluator.evaluate_conversation(conv_id, conv_dir)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        # Calculate overall statistics and save results\n",
    "        evaluator._calculate_overall_statistics(results)\n",
    "        evaluator._save_evaluation_results(results)\n",
    "        evaluator._generate_evaluation_report(results)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e6d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting comprehensive evaluation...\n",
      " Found 1 conversations to evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating conversations: 100%|██████████| 1/1 [00:00<00:00, 88.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating Conversation 1\n",
      "   Ground truth loaded: 4 speakers\n",
      "   Pipeline output loaded: 4 speaker files\n",
      "   Ground truth segments extracted for speakers: ['7021', '7729', '1284', '4992']\n",
      "    7021: 3 segments, 27.7s total\n",
      "    7729: 3 segments, 31.4s total\n",
      "    1284: 3 segments, 20.6s total\n",
      "    4992: 3 segments, 20.1s total\n",
      "   Evaluating 7021...\n",
      "    Found 3 predicted segments\n",
      "    WER: 18.57%\n",
      "    Segments: 3\n",
      "   Evaluating 7729...\n",
      "    Found 2 predicted segments\n",
      "    WER: 10.13%\n",
      "    Segments: 2\n",
      "   Evaluating 1284...\n",
      "    Found 2 predicted segments\n",
      "    WER: 30.00%\n",
      "    Segments: 2\n",
      "   Evaluating 4992...\n",
      "    Found 3 predicted segments\n",
      "    WER: 18.75%\n",
      "    Segments: 3\n",
      "   Timeline has 10 segments\n",
      "   Overall WER: 19.36%\n",
      "   Diarization Accuracy: 89.03%\n",
      "   Speaker Detection: 4/4\n",
      " Detailed results saved to: evaluation_results\\detailed_evaluation_results.json\n",
      " Evaluation report saved to: evaluation_results\\evaluation_report.txt\n",
      "\n",
      " EVALUATION SUMMARY\n",
      "==================================================\n",
      "Mean WER: 19.36%\n",
      "Mean Diarization Accuracy: 89.03%\n",
      "Mean Speaker Detection Rate: 100.00%\n",
      "Total Conversations: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = evaluate_pipeline_results(\n",
    "    ground_truth_path=\"Pipeline_data/Conv/ver1/conversations_metadata.json\",\n",
    "    pipeline_output_dir=\"pipeline_data/Conv/ver1/seperated_transcriptions\",\n",
    "    output_dir=\"evaluation_results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9bfd2",
   "metadata": {},
   "source": [
    "## End to End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c39aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from src.seperator import VoiceSeparator\n",
    "from utils.audio import Audio\n",
    "from utils.hparams import HParam\n",
    "from model.embedder import SpeechEmbedder\n",
    "\n",
    "\n",
    "class AudioSeparatorTranscriber:\n",
    "    \"\"\"\n",
    "    Ready-made setup for audio separation and transcription.\n",
    "    Takes a mixed WAV file and reference audio paths, then processes everything.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, separator_config, asr_config, processing_params=None):\n",
    "        \"\"\"\n",
    "        Initialize the separator and transcriber.\n",
    "\n",
    "        Args:\n",
    "            separator_config: Dict with separator model paths\n",
    "            asr_config: Dict with ASR model configuration\n",
    "            processing_params: Dict with processing parameters\n",
    "        \"\"\"\n",
    "        self.separator_config = separator_config\n",
    "        self.asr_config = asr_config\n",
    "        self.target_sr = 16000\n",
    "        self.chunk_size = 3.0  # seconds\n",
    "        self.overlap = 0.05  # seconds\n",
    "\n",
    "        # Processing parameters\n",
    "        default_params = {\n",
    "            \"similarity_threshold\": 0.3,  # D-vector similarity threshold\n",
    "            \"energy_threshold\": 0.001,  # Audio energy threshold\n",
    "            \"min_text_length\": 2,  # Minimum text length\n",
    "            \"confidence_threshold\": 0.0,  # ASR confidence threshold\n",
    "            \"time_gap_threshold\": 1.0,  # Time gap for merging segments\n",
    "            \"min_audio_length\": 0.5,  # Minimum audio length for d-vector\n",
    "            \"contrast_threshold\": 0.1,  # Minimum contrast between speakers\n",
    "            \"debug_mode\": False,  # Show detailed debug info\n",
    "        }\n",
    "\n",
    "        self.params = default_params.copy()\n",
    "        if processing_params:\n",
    "            self.params.update(processing_params)\n",
    "\n",
    "        # Initialize models\n",
    "        self._init_separator()\n",
    "        self._init_asr()\n",
    "        self._init_embedder()\n",
    "\n",
    "        # Cache for reference d-vectors\n",
    "        self.reference_dvecs = {}\n",
    "\n",
    "    def _init_separator(self):\n",
    "        \"\"\"Initialize voice separator\"\"\"\n",
    "        self.separator = VoiceSeparator(\n",
    "            config_path=self.separator_config[\"config_path\"],\n",
    "            embedder_path=self.separator_config[\"embedder_path\"],\n",
    "            checkpoint_path=self.separator_config[\"checkpoint_path\"],\n",
    "            return_dvec=self.separator_config.get(\"return_dvec\", False),\n",
    "        )\n",
    "\n",
    "    def _init_asr(self):\n",
    "        \"\"\"Initialize ASR pipeline\"\"\"\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.stt_pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=self.asr_config[\"model\"],\n",
    "            device=device,\n",
    "            **self.asr_config.get(\"init_kwargs\", {}),\n",
    "        )\n",
    "\n",
    "    def _init_embedder(self):\n",
    "        \"\"\"Initialize speech embedder for d-vector extraction\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.hp = HParam(self.separator_config[\"config_path\"])\n",
    "        self.embedder = SpeechEmbedder(self.hp).to(device)\n",
    "        self.embedder.load_state_dict(\n",
    "            torch.load(self.separator_config[\"embedder_path\"], map_location=device)\n",
    "        )\n",
    "        self.embedder.eval()\n",
    "        self.audio_processor = Audio(self.hp)\n",
    "        self.device = device\n",
    "\n",
    "    def preprocess_audio(self, audio_path, output_path=None):\n",
    "        \"\"\"\n",
    "        Preprocess audio: load, resample to 16kHz, normalize\n",
    "\n",
    "        Args:\n",
    "            audio_path: Path to input audio file\n",
    "            output_path: Optional path to save preprocessed audio\n",
    "\n",
    "        Returns:\n",
    "            audio_data: Preprocessed audio array\n",
    "            duration: Audio duration in seconds\n",
    "        \"\"\"\n",
    "        print(f\"Preprocessing audio: {Path(audio_path).name}\")\n",
    "\n",
    "        # Load audio\n",
    "        audio_data, original_sr = librosa.load(audio_path, sr=None)\n",
    "        print(f\"  Original: {original_sr}Hz, {len(audio_data)/original_sr:.2f}s\")\n",
    "\n",
    "        # Resample to 16kHz if needed\n",
    "        if original_sr != self.target_sr:\n",
    "            audio_data = librosa.resample(\n",
    "                audio_data, orig_sr=original_sr, target_sr=self.target_sr\n",
    "            )\n",
    "            print(f\"  Resampled to {self.target_sr}Hz\")\n",
    "\n",
    "        # Normalize audio\n",
    "        audio_data = audio_data / np.max(np.abs(audio_data))\n",
    "        print(f\"  Normalized to [-1, 1]\")\n",
    "\n",
    "        duration = len(audio_data) / self.target_sr\n",
    "        print(f\"  Final: {self.target_sr}Hz, {duration:.2f}s\")\n",
    "\n",
    "        # Save preprocessed audio if requested\n",
    "        if output_path:\n",
    "            sf.write(output_path, audio_data, self.target_sr)\n",
    "            print(f\"  Saved to: {output_path}\")\n",
    "\n",
    "        return audio_data, duration\n",
    "\n",
    "    def extract_dvec(self, audio_path):\n",
    "        \"\"\"Extract d-vector from audio file\"\"\"\n",
    "        wav, _ = librosa.load(audio_path, sr=self.target_sr)\n",
    "        mel = self.audio_processor.get_mel(wav)\n",
    "        mel = torch.from_numpy(mel).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            return self.embedder(mel).unsqueeze(0)\n",
    "\n",
    "    def prepare_references(self, reference_paths):\n",
    "        \"\"\"\n",
    "        Prepare reference audio files and extract d-vectors\n",
    "\n",
    "        Args:\n",
    "            reference_paths: Dict {speaker_id: audio_path} or List of audio paths\n",
    "\n",
    "        Returns:\n",
    "            reference_info: Dict with speaker info and d-vectors\n",
    "        \"\"\"\n",
    "        print(\"\\nPreparing reference audio files...\")\n",
    "\n",
    "        \n",
    "        if isinstance(reference_paths, list):\n",
    "            # Convert list to dict with speaker IDs\n",
    "            reference_info = {\n",
    "                f\"Speaker_{i+1}\": path for i, path in enumerate(reference_paths)\n",
    "            }\n",
    "        elif isinstance(reference_paths, dict):\n",
    "            reference_info = reference_paths.copy()\n",
    "        else:\n",
    "            raise ValueError(\"reference_paths must be a list or dict\")\n",
    "\n",
    "        # Extract d-vectors for each reference\n",
    "        for speaker_id, ref_path in reference_info.items():\n",
    "            if not os.path.exists(ref_path):\n",
    "                raise FileNotFoundError(f\"Reference audio not found: {ref_path}\")\n",
    "\n",
    "            # Preprocess reference audio\n",
    "            ref_audio, _ = self.preprocess_audio(ref_path)\n",
    "\n",
    "            # Save preprocessed version temporarily\n",
    "            temp_ref_path = f\"/tmp/ref_{speaker_id}.wav\"\n",
    "            sf.write(temp_ref_path, ref_audio, self.target_sr)\n",
    "\n",
    "            # Extract d-vector\n",
    "            self.reference_dvecs[speaker_id] = self.extract_dvec(temp_ref_path)\n",
    "\n",
    "            if os.path.exists(temp_ref_path):\n",
    "                os.remove(temp_ref_path)\n",
    "\n",
    "            print(f\"   {speaker_id}: {Path(ref_path).name}\")\n",
    "\n",
    "        return reference_info\n",
    "\n",
    "    def chunk_audio(self, audio_data):\n",
    "        \"\"\"Split audio into overlapping chunks\"\"\"\n",
    "        chunk_samples = int(self.chunk_size * self.target_sr)\n",
    "        hop_samples = int((self.chunk_size - self.overlap) * self.target_sr)\n",
    "\n",
    "        chunks = []\n",
    "        for i in range(0, len(audio_data) - chunk_samples + 1, hop_samples):\n",
    "            chunk = audio_data[i : i + chunk_samples]\n",
    "            start_time = i / self.target_sr\n",
    "            end_time = (i + chunk_samples) / self.target_sr\n",
    "\n",
    "            chunks.append(\n",
    "                {\n",
    "                    \"data\": chunk,\n",
    "                    \"start_time\": start_time,\n",
    "                    \"end_time\": end_time,\n",
    "                    \"chunk_id\": len(chunks),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def separate_chunk(self, chunk, reference_info, temp_dir=\"/tmp\"):\n",
    "        \"\"\"Separate audio chunk for all speakers\"\"\"\n",
    "        chunk_path = f\"{temp_dir}/chunk_{chunk['chunk_id']}.wav\"\n",
    "        sf.write(chunk_path, chunk[\"data\"], self.target_sr)\n",
    "\n",
    "        separated_audios = {}\n",
    "\n",
    "        for speaker_id, ref_path in reference_info.items():\n",
    "            try:\n",
    "                temp_ref_path = f\"{temp_dir}/temp_ref_{speaker_id}.wav\"\n",
    "                ref_audio, _ = librosa.load(ref_path, sr=self.target_sr)\n",
    "                ref_audio = ref_audio / np.max(np.abs(ref_audio))  # Normalize\n",
    "                sf.write(temp_ref_path, ref_audio, self.target_sr)\n",
    "\n",
    "                # Separate audio\n",
    "                est_audio, dvec = self.separator.separate(\n",
    "                    reference_file=temp_ref_path,\n",
    "                    mixed_file=chunk_path,\n",
    "                    out_dir=None,\n",
    "                )\n",
    "\n",
    "                separated_audios[speaker_id] = est_audio\n",
    "\n",
    "                if os.path.exists(temp_ref_path):\n",
    "                    os.remove(temp_ref_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                if self.params[\"debug_mode\"]:\n",
    "                    print(f\"Separation failed for {speaker_id}: {e}\")\n",
    "                separated_audios[speaker_id] = np.zeros_like(chunk[\"data\"])\n",
    "\n",
    "        # Clean up chunk file\n",
    "        if os.path.exists(chunk_path):\n",
    "            os.remove(chunk_path)\n",
    "\n",
    "        return separated_audios\n",
    "\n",
    "    def identify_active_speakers(self, separated_audios, chunk_info):\n",
    "        \"\"\"Identify active speakers using d-vector similarity\"\"\"\n",
    "        active_speakers = {}\n",
    "\n",
    "        if self.params[\"debug_mode\"]:\n",
    "            print(f\"    Analyzing chunk {chunk_info['chunk_id']}:\")\n",
    "\n",
    "        for speaker_id, separated_audio in separated_audios.items():\n",
    "            if len(separated_audio) < self.params[\"min_audio_length\"] * self.target_sr:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Extract d-vector from separated audio\n",
    "                mel = self.audio_processor.get_mel(separated_audio)\n",
    "                mel_tensor = torch.from_numpy(mel).float().to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    chunk_dvec = self.embedder(mel_tensor).unsqueeze(0)\n",
    "\n",
    "                # Compare with reference\n",
    "                ref_dvec = self.reference_dvecs[speaker_id]\n",
    "                similarity = torch.nn.functional.cosine_similarity(\n",
    "                    chunk_dvec, ref_dvec\n",
    "                ).item()\n",
    "\n",
    "                # Calculate contrast with other speakers\n",
    "                other_similarities = []\n",
    "                for other_speaker_id in self.reference_dvecs.keys():\n",
    "                    if other_speaker_id != speaker_id:\n",
    "                        other_ref = self.reference_dvecs[other_speaker_id]\n",
    "                        other_sim = torch.nn.functional.cosine_similarity(\n",
    "                            chunk_dvec, other_ref\n",
    "                        ).item()\n",
    "                        other_similarities.append(other_sim)\n",
    "\n",
    "                max_other_sim = max(other_similarities) if other_similarities else 0.0\n",
    "                contrast = similarity - max_other_sim\n",
    "\n",
    "                # Calculate energy\n",
    "                energy = np.mean(separated_audio**2)\n",
    "\n",
    "                # Check thresholds\n",
    "                if (\n",
    "                    similarity > self.params[\"similarity_threshold\"]\n",
    "                    and contrast > self.params[\"contrast_threshold\"]\n",
    "                    and energy > self.params[\"energy_threshold\"]\n",
    "                ):\n",
    "\n",
    "                    active_speakers[speaker_id] = {\n",
    "                        \"audio\": separated_audio,\n",
    "                        \"similarity\": similarity,\n",
    "                        \"contrast\": contrast,\n",
    "                        \"energy\": energy,\n",
    "                    }\n",
    "\n",
    "                    if self.params[\"debug_mode\"]:\n",
    "                        print(\n",
    "                            f\"      {speaker_id}: ACTIVE (sim={similarity:.3f}, \"\n",
    "                            f\"contrast={contrast:.3f}, energy={energy:.6f})\"\n",
    "                        )\n",
    "                elif self.params[\"debug_mode\"]:\n",
    "                    print(\n",
    "                        f\"      {speaker_id}: inactive (sim={similarity:.3f}, \"\n",
    "                        f\"contrast={contrast:.3f}, energy={energy:.6f})\"\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                if self.params[\"debug_mode\"]:\n",
    "                    print(f\"      {speaker_id}: Error in analysis: {e}\")\n",
    "\n",
    "        return active_speakers\n",
    "\n",
    "    def transcribe_speakers(self, active_speakers, chunk_info):\n",
    "        \"\"\"Transcribe audio for active speakers\"\"\"\n",
    "        transcriptions = []\n",
    "\n",
    "        for speaker_id, speaker_data in active_speakers.items():\n",
    "            try:\n",
    "                result = self.stt_pipe(speaker_data[\"audio\"])\n",
    "                text = result[\"text\"].strip()\n",
    "                confidence = result.get(\"confidence\", 1.0)\n",
    "\n",
    "                if self._is_valid_text(text, confidence):\n",
    "                    transcriptions.append(\n",
    "                        {\n",
    "                            \"speaker_id\": speaker_id,\n",
    "                            \"start_time\": chunk_info[\"start_time\"],\n",
    "                            \"end_time\": chunk_info[\"end_time\"],\n",
    "                            \"text\": text,\n",
    "                            \"confidence\": confidence,\n",
    "                            \"similarity\": speaker_data[\"similarity\"],\n",
    "                            \"contrast\": speaker_data[\"contrast\"],\n",
    "                            \"energy\": speaker_data[\"energy\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    if self.params[\"debug_mode\"]:\n",
    "                        print(f\"      {speaker_id}: '{text}' (conf={confidence:.3f})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                if self.params[\"debug_mode\"]:\n",
    "                    print(f\"      {speaker_id}: Transcription error: {e}\")\n",
    "\n",
    "        return transcriptions\n",
    "\n",
    "    def _is_valid_text(self, text, confidence):\n",
    "        \"\"\"Check if transcribed text is valid\"\"\"\n",
    "        if not text or len(text) < self.params[\"min_text_length\"]:\n",
    "            return False\n",
    "\n",
    "        if confidence < self.params[\"confidence_threshold\"]:\n",
    "            return False\n",
    "\n",
    "        # Filter noise patterns\n",
    "        text_lower = text.lower().strip()\n",
    "        noise_patterns = [\n",
    "            r\"^[\\s\\-\\.]+$\",\n",
    "            r\"^(uh|um|ah|er|mm|hmm)[\\s\\-]*$\",\n",
    "            r\"^\\[.*\\]$\",\n",
    "            r\"^[\\(\\)]+$\",\n",
    "        ]\n",
    "\n",
    "        for pattern in noise_patterns:\n",
    "            if re.match(pattern, text_lower):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def merge_segments(self, transcriptions):\n",
    "        \"\"\"Merge consecutive segments from same speaker\"\"\"\n",
    "        if not transcriptions:\n",
    "            return []\n",
    "\n",
    "        # Group by speaker\n",
    "        speaker_segments = {}\n",
    "        for trans in transcriptions:\n",
    "            speaker_id = trans[\"speaker_id\"]\n",
    "            if speaker_id not in speaker_segments:\n",
    "                speaker_segments[speaker_id] = []\n",
    "            speaker_segments[speaker_id].append(trans)\n",
    "\n",
    "        # Merge segments for each speaker\n",
    "        merged_segments = []\n",
    "        for speaker_id, segments in speaker_segments.items():\n",
    "            segments.sort(key=lambda x: x[\"start_time\"])\n",
    "\n",
    "            if not segments:\n",
    "                continue\n",
    "\n",
    "            current = segments[0].copy()\n",
    "\n",
    "            for next_seg in segments[1:]:\n",
    "                time_gap = next_seg[\"start_time\"] - current[\"end_time\"]\n",
    "\n",
    "                if time_gap <= self.params[\"time_gap_threshold\"]:\n",
    "                    # Merge segments\n",
    "                    current[\"end_time\"] = next_seg[\"end_time\"]\n",
    "                    current[\"text\"] += \" \" + next_seg[\"text\"]\n",
    "                    current[\"confidence\"] = max(\n",
    "                        current[\"confidence\"], next_seg[\"confidence\"]\n",
    "                    )\n",
    "                else:\n",
    "                    # Start new segment\n",
    "                    merged_segments.append(current)\n",
    "                    current = next_seg.copy()\n",
    "\n",
    "            merged_segments.append(current)\n",
    "\n",
    "        # Sort all segments by time\n",
    "        merged_segments.sort(key=lambda x: x[\"start_time\"])\n",
    "        return merged_segments\n",
    "\n",
    "    def process(self, mixed_audio_path, reference_paths, output_dir=\"output\"):\n",
    "        \"\"\"\n",
    "        Main processing function\n",
    "\n",
    "        Args:\n",
    "            mixed_audio_path: Path to mixed audio file\n",
    "            reference_paths: Dict {speaker_id: ref_path} or List of ref paths\n",
    "            output_dir: Output directory for results\n",
    "\n",
    "        Returns:\n",
    "            results: Dict with transcription results\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"AUDIO SEPARATOR AND TRANSCRIBER\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Create output directory\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Step 1: Preprocess mixed audio\n",
    "        print(\"\\n1. PREPROCESSING MIXED AUDIO\")\n",
    "        mixed_audio_data, duration = self.preprocess_audio(mixed_audio_path)\n",
    "\n",
    "        # Step 2: Prepare references\n",
    "        print(\"\\n2. PREPARING REFERENCE AUDIO\")\n",
    "        reference_info = self.prepare_references(reference_paths)\n",
    "\n",
    "        # Step 3: Process audio in chunks\n",
    "        print(f\"\\n3. PROCESSING AUDIO ({duration:.1f}s)\")\n",
    "        chunks = self.chunk_audio(mixed_audio_data)\n",
    "        print(f\"Created {len(chunks)} chunks ({self.chunk_size}s each)\")\n",
    "\n",
    "        all_transcriptions = []\n",
    "\n",
    "        for chunk in tqdm(chunks, desc=\"Processing chunks\"):\n",
    "            if self.params[\"debug_mode\"]:\n",
    "                print(\n",
    "                    f\"\\n  Chunk {chunk['chunk_id']} \"\n",
    "                    f\"[{chunk['start_time']:.1f}s - {chunk['end_time']:.1f}s]:\"\n",
    "                )\n",
    "\n",
    "            # Separate audio\n",
    "            separated_audios = self.separate_chunk(chunk, reference_info)\n",
    "\n",
    "            # Identify active speakers\n",
    "            active_speakers = self.identify_active_speakers(separated_audios, chunk)\n",
    "\n",
    "            # Transcribe active speakers\n",
    "            if active_speakers:\n",
    "                transcriptions = self.transcribe_speakers(active_speakers, chunk)\n",
    "                all_transcriptions.extend(transcriptions)\n",
    "\n",
    "        # Step 4: Merge consecutive segments\n",
    "        print(f\"\\n4. MERGING SEGMENTS\")\n",
    "        merged_transcriptions = self.merge_segments(all_transcriptions)\n",
    "        print(\n",
    "            f\"Merged {len(all_transcriptions)} → {len(merged_transcriptions)} segments\"\n",
    "        )\n",
    "\n",
    "        # Step 5: Save results\n",
    "        print(f\"\\n5. SAVING RESULTS\")\n",
    "        results = self._save_results(merged_transcriptions, reference_info, output_dir)\n",
    "\n",
    "        print(f\"\\nPROCESSING COMPLETE!\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _save_results(self, transcriptions, reference_info, output_dir):\n",
    "        \"\"\"Save transcription results\"\"\"\n",
    "        # Create timeline\n",
    "        timeline = []\n",
    "        speaker_stats = {}\n",
    "\n",
    "        for trans in transcriptions:\n",
    "            speaker_id = trans[\"speaker_id\"]\n",
    "\n",
    "            timeline.append(\n",
    "                {\n",
    "                    \"speaker\": speaker_id,\n",
    "                    \"start_time\": trans[\"start_time\"],\n",
    "                    \"end_time\": trans[\"end_time\"],\n",
    "                    \"duration\": trans[\"end_time\"] - trans[\"start_time\"],\n",
    "                    \"text\": trans[\"text\"],\n",
    "                    \"confidence\": trans[\"confidence\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Update speaker stats\n",
    "            if speaker_id not in speaker_stats:\n",
    "                speaker_stats[speaker_id] = {\n",
    "                    \"total_segments\": 0,\n",
    "                    \"total_duration\": 0.0,\n",
    "                    \"segments\": [],\n",
    "                }\n",
    "\n",
    "            speaker_stats[speaker_id][\"total_segments\"] += 1\n",
    "            speaker_stats[speaker_id][\"total_duration\"] += (\n",
    "                trans[\"end_time\"] - trans[\"start_time\"]\n",
    "            )\n",
    "            speaker_stats[speaker_id][\"segments\"].append(\n",
    "                {\n",
    "                    \"start_time\": trans[\"start_time\"],\n",
    "                    \"end_time\": trans[\"end_time\"],\n",
    "                    \"text\": trans[\"text\"],\n",
    "                    \"confidence\": trans[\"confidence\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Save main results\n",
    "        results = {\n",
    "            \"processing_info\": {\n",
    "                \"total_segments\": len(transcriptions),\n",
    "                \"speakers\": list(reference_info.keys()),\n",
    "                \"parameters\": self.params,\n",
    "            },\n",
    "            \"speaker_statistics\": speaker_stats,\n",
    "            \"timeline\": timeline,\n",
    "        }\n",
    "\n",
    "        # Save JSON\n",
    "        results_path = output_dir / \"transcription_results.json\"\n",
    "        with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  Results: {results_path}\")\n",
    "\n",
    "        # Save individual speaker files\n",
    "        for speaker_id, stats in speaker_stats.items():\n",
    "            speaker_path = output_dir / f\"{speaker_id}_transcription.json\"\n",
    "            with open(speaker_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(\n",
    "                    {\n",
    "                        \"speaker_id\": speaker_id,\n",
    "                        \"total_segments\": stats[\"total_segments\"],\n",
    "                        \"total_duration\": stats[\"total_duration\"],\n",
    "                        \"segments\": stats[\"segments\"],\n",
    "                    },\n",
    "                    f,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                )\n",
    "            print(f\"  {speaker_id}: {speaker_path}\")\n",
    "\n",
    "        # Save readable transcript\n",
    "        transcript_path = output_dir / \"transcript.txt\"\n",
    "        with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"AUDIO TRANSCRIPTION\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "            for item in timeline:\n",
    "                f.write(\n",
    "                    f\"[{item['start_time']:6.1f}s - {item['end_time']:6.1f}s] \"\n",
    "                    f\"{item['speaker']}: {item['text']}\\n\"\n",
    "                )\n",
    "        print(f\"  Transcript: {transcript_path}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def process_audio(\n",
    "    mixed_audio_path,\n",
    "    reference_paths,\n",
    "    separator_config,\n",
    "    asr_config,\n",
    "    processing_params=None,\n",
    "    output_dir=\"output\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Convenience function to process audio with default settings\n",
    "\n",
    "    Args:\n",
    "        mixed_audio_path: Path to mixed audio file\n",
    "        reference_paths: Dict {speaker_id: ref_path} or List of ref paths\n",
    "        separator_config: Separator model configuration\n",
    "        asr_config: ASR model configuration\n",
    "        processing_params: Optional processing parameters\n",
    "        output_dir: Output directory\n",
    "\n",
    "    Returns:\n",
    "        results: Transcription results\n",
    "    \"\"\"\n",
    "    processor = AudioSeparatorTranscriber(\n",
    "        separator_config=separator_config,\n",
    "        asr_config=asr_config,\n",
    "        processing_params=processing_params,\n",
    "    )\n",
    "\n",
    "    return processor.process(mixed_audio_path, reference_paths, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "278dc306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AUDIO SEPARATOR AND TRANSCRIBER\n",
      "============================================================\n",
      "\n",
      "1. PREPROCESSING MIXED AUDIO\n",
      "Preprocessing audio: conversation_000.wav\n",
      "  Original: 16000Hz, 84.44s\n",
      "  Normalized to [-1, 1]\n",
      "  Final: 16000Hz, 84.44s\n",
      "\n",
      "2. PREPARING REFERENCE AUDIO\n",
      "\n",
      "Preparing reference audio files...\n",
      "Preprocessing audio: 8463.wav\n",
      "  Original: 16000Hz, 5.98s\n",
      "  Normalized to [-1, 1]\n",
      "  Final: 16000Hz, 5.98s\n",
      "   Speaker_A: 8463.wav\n",
      "Preprocessing audio: 6829.wav\n",
      "  Original: 16000Hz, 5.32s\n",
      "  Normalized to [-1, 1]\n",
      "  Final: 16000Hz, 5.32s\n",
      "   Speaker_B: 6829.wav\n",
      "Preprocessing audio: 7021.wav\n",
      "  Original: 16000Hz, 18.41s\n",
      "  Normalized to [-1, 1]\n",
      "  Final: 16000Hz, 18.41s\n",
      "   Speaker_C: 7021.wav\n",
      "Preprocessing audio: 8224.wav\n",
      "  Original: 16000Hz, 26.16s\n",
      "  Normalized to [-1, 1]\n",
      "  Final: 16000Hz, 26.16s\n",
      "   Speaker_D: 8224.wav\n",
      "\n",
      "3. PROCESSING AUDIO (84.4s)\n",
      "Created 28 chunks (3.0s each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/28 [00:00<?, ?it/s]c:\\Users\\USER\\anaconda3\\envs\\vsep\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Processing chunks: 100%|██████████| 28/28 [00:38<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. MERGING SEGMENTS\n",
      "Merged 35 → 10 segments\n",
      "\n",
      "5. SAVING RESULTS\n",
      "  Results: Pipeline_data\\Conv\\ver1\\end_to_end_results\\transcription_results.json\n",
      "  Speaker_D: Pipeline_data\\Conv\\ver1\\end_to_end_results\\Speaker_D_transcription.json\n",
      "  Speaker_C: Pipeline_data\\Conv\\ver1\\end_to_end_results\\Speaker_C_transcription.json\n",
      "  Speaker_B: Pipeline_data\\Conv\\ver1\\end_to_end_results\\Speaker_B_transcription.json\n",
      "  Speaker_A: Pipeline_data\\Conv\\ver1\\end_to_end_results\\Speaker_A_transcription.json\n",
      "  Transcript: Pipeline_data\\Conv\\ver1\\end_to_end_results\\transcript.txt\n",
      "\n",
      "PROCESSING COMPLETE!\n",
      "Output directory: Pipeline_data\\Conv\\ver1\\end_to_end_results\n",
      "Processing completed!\n",
      "Found 10 segments\n",
      "Speaker_D: 2 segments, 41.4s total\n",
      "Speaker_C: 3 segments, 26.7s total\n",
      "Speaker_B: 3 segments, 20.8s total\n",
      "Speaker_A: 2 segments, 14.8s total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "separator_config = {\n",
    "    \"config_path\": \"config/inference.yaml\",\n",
    "    \"embedder_path\": \"ckpt/embedder.pt\",\n",
    "    \"checkpoint_path\": \"ckpt/seperator_best_checkpoint.pt\",\n",
    "    \"return_dvec\": False,\n",
    "}\n",
    "\n",
    "asr_config = {\n",
    "    \"model\": \"ckpt/whisper-small\",\n",
    "    \"init_kwargs\": {},\n",
    "}\n",
    "\n",
    "processing_params = {\n",
    "    \"similarity_threshold\": 0.2,\n",
    "    \"energy_threshold\": 0.0005,\n",
    "    \"debug_mode\": False,\n",
    "}\n",
    "\n",
    "\n",
    "results = process_audio(\n",
    "    mixed_audio_path=\"pipeline_data/Conv/ver1/conversations/conversation_000.wav\",\n",
    "    reference_paths={\n",
    "        \"Speaker_A\": \"Pipeline_data/Conv/ver1/reference/8463.wav\",\n",
    "        \"Speaker_B\": \"Pipeline_data/Conv/ver1/reference/6829.wav\",\n",
    "        \"Speaker_C\": \"Pipeline_data/Conv/ver1/reference/7021.wav\",\n",
    "        \"Speaker_D\": \"Pipeline_data/Conv/ver1/reference/8224.wav\",\n",
    "    },\n",
    "    separator_config=separator_config,\n",
    "    asr_config=asr_config,\n",
    "    processing_params=processing_params,\n",
    "    output_dir=\"Pipeline_data/Conv/ver1/end_to_end_results\",\n",
    ")\n",
    "\n",
    "print(\"Processing completed!\")\n",
    "print(f\"Found {len(results['timeline'])} segments\")\n",
    "for speaker, stats in results[\"speaker_statistics\"].items():\n",
    "    print(\n",
    "        f\"{speaker}: {stats['total_segments']} segments, \"\n",
    "        f\"{stats['total_duration']:.1f}s total\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
